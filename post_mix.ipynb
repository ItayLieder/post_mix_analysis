{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a5cca9-1c7d-4246-8556-00b077b1d349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Mix I/O layer loaded: AudioBuffer, load_wav, save_wav, resample_poly, slice_preview, with_suffix, auto_out_path, sha256_file, print_audio_summary.\n"
     ]
    }
   ],
   "source": [
    "# Post-Mix I/O Layer — Notebook Version\n",
    "# - WAV-focused (robust float32 pipeline; no external deps)\n",
    "# - Safe loading/saving, resampling, preview slicing, hashing, path helpers\n",
    "# - Designed to later lift into .py modules with minimal changes\n",
    "#\n",
    "# Note: This cell only defines functions/classes. No files are written.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "import os\n",
    "import math\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ----------------------------- Dataclasses -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class AudioBuffer:\n",
    "    \"\"\"In-memory audio container standardized to float32 in [-1, 1].\"\"\"\n",
    "    sr: int\n",
    "    samples: np.ndarray  # shape: (N,) mono or (N, 2) stereo (float32)\n",
    "    path: Optional[str] = None\n",
    "    meta: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    @property\n",
    "    def n_samples(self) -> int:\n",
    "        return int(self.samples.shape[0])\n",
    "\n",
    "    @property\n",
    "    def n_channels(self) -> int:\n",
    "        return 1 if self.samples.ndim == 1 else int(self.samples.shape[1])\n",
    "\n",
    "    @property\n",
    "    def duration_s(self) -> float:\n",
    "        return float(self.n_samples) / float(self.sr if self.sr else 1)\n",
    "\n",
    "    @property\n",
    "    def peak(self) -> float:\n",
    "        return float(np.max(np.abs(self.samples))) if self.n_samples else 0.0\n",
    "\n",
    "    @property\n",
    "    def rms(self) -> float:\n",
    "        return float(np.sqrt(np.mean(np.square(self.samples)))) if self.n_samples else 0.0\n",
    "\n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"sr\": self.sr,\n",
    "            \"channels\": self.n_channels,\n",
    "            \"duration_s\": round(self.duration_s, 3),\n",
    "            \"peak\": round(self.peak, 6),\n",
    "            \"rms\": round(self.rms, 6),\n",
    "            \"path\": self.path,\n",
    "            \"meta\": self.meta or {},\n",
    "        }\n",
    "\n",
    "\n",
    "# ----------------------------- Helpers -----------------------------\n",
    "\n",
    "def _to_float32(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert common PCM/float types to float32 in [-1, 1].\"\"\"\n",
    "    if x.dtype == np.int16:\n",
    "        y = x.astype(np.float32) / 32768.0\n",
    "    elif x.dtype == np.int32:\n",
    "        y = x.astype(np.float32) / 2147483648.0\n",
    "    elif x.dtype == np.uint8:\n",
    "        # 8-bit WAV is offset binary [0..255]\n",
    "        y = (x.astype(np.float32) - 128.0) / 128.0\n",
    "    elif x.dtype in (np.float32, np.float64):\n",
    "        y = x.astype(np.float32)\n",
    "        # Assume already -1..1 but sanitize below\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported WAV dtype: {x.dtype}\")\n",
    "    # Sanitize\n",
    "    y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Clip extreme outliers for safety\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _ensure_stereo(x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim == 1:\n",
    "        return np.stack([x, x], axis=-1)\n",
    "    if x.shape[1] == 1:\n",
    "        return np.repeat(x, 2, axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _downmix_mono(x: np.ndarray) -> np.ndarray:\n",
    "    return x if x.ndim == 1 else np.mean(x, axis=1).astype(np.float32)\n",
    "\n",
    "\n",
    "def tpdf_dither_16bit(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Triangular PDF dithering to 16-bit PCM.\"\"\"\n",
    "    x = np.clip(x, -1.0, 1.0)\n",
    "    lsb = 1.0 / 32768.0\n",
    "    noise = (np.random.rand(*x.shape).astype(np.float32) - np.random.rand(*x.shape).astype(np.float32)) * lsb\n",
    "    y = x + noise\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "    return np.int16(np.round(y * 32767.0))\n",
    "\n",
    "\n",
    "def resample_poly(x: np.ndarray, sr_in: int, sr_out: int) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"High-quality rational resampling with polyphase filtering.\"\"\"\n",
    "    if sr_in == sr_out:\n",
    "        return x, sr_in\n",
    "    gcd = math.gcd(sr_in, sr_out)\n",
    "    up = sr_out // gcd\n",
    "    down = sr_in // gcd\n",
    "    y = signal.resample_poly(x, up, down, axis=0 if x.ndim > 1 else 0)\n",
    "    return y.astype(np.float32), sr_out\n",
    "\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    \"\"\"Create parent dir for a file path if missing.\"\"\"\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "\n",
    "\n",
    "def with_suffix(input_path: str, suffix: str, ext: Optional[str] = None) -> str:\n",
    "    \"\"\"Returns a new path by inserting a suffix before extension (or switching ext).\"\"\"\n",
    "    d, base = os.path.split(input_path)\n",
    "    stem, old_ext = os.path.splitext(base)\n",
    "    new_ext = ext if ext else old_ext\n",
    "    name = f\"{stem}{suffix}{new_ext}\"\n",
    "    return os.path.join(d, name)\n",
    "\n",
    "\n",
    "def sha256_file(path: str, chunk_size: int = 1 << 20) -> str:\n",
    "    \"\"\"Compute SHA-256 for reproducibility tracking.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def slice_preview(buf: AudioBuffer, start_s: float = 0.0, dur_s: float = 30.0) -> AudioBuffer:\n",
    "    \"\"\"Return a window for fast preview rendering.\"\"\"\n",
    "    n0 = max(0, int(start_s * buf.sr))\n",
    "    n1 = min(buf.n_samples, n0 + int(dur_s * buf.sr))\n",
    "    sl = buf.samples[n0:n1].copy()\n",
    "    return AudioBuffer(sr=buf.sr, samples=sl, path=buf.path, meta={\"slice\": [start_s, dur_s], **(buf.meta or {})})\n",
    "\n",
    "\n",
    "# ----------------------------- I/O API -----------------------------\n",
    "\n",
    "def load_wav(path: str,\n",
    "             target_sr: Optional[int] = None,\n",
    "             mono: bool = False,\n",
    "             sanitize: bool = True) -> AudioBuffer:\n",
    "    \"\"\"Load a WAV into float32[-1,1], optional resample & mono downmix.\"\"\"\n",
    "    sr, data = wavfile.read(path)\n",
    "    y = _to_float32(data)\n",
    "    if mono:\n",
    "        y = _downmix_mono(y)\n",
    "    # Resample if needed\n",
    "    if target_sr is not None and target_sr != sr:\n",
    "        y, sr = resample_poly(y, sr, target_sr)\n",
    "    # Final sanitize\n",
    "    if sanitize:\n",
    "        y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = np.clip(y, -1.0, 1.0)\n",
    "    # Pack\n",
    "    meta = {\n",
    "        \"loaded_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"source_sha256\": sha256_file(path),\n",
    "        \"dtype_in\": str(data.dtype),\n",
    "        \"channels_in\": 1 if data.ndim == 1 else int(data.shape[1]),\n",
    "    }\n",
    "    return AudioBuffer(sr=sr, samples=y, path=os.path.abspath(path), meta=meta)\n",
    "\n",
    "\n",
    "def save_wav(path: str,\n",
    "             buf: AudioBuffer | np.ndarray,\n",
    "             sr: Optional[int] = None,\n",
    "             bitdepth: str = \"float32\",\n",
    "             dither_16bit: bool = True) -> str:\n",
    "    \"\"\"Save an AudioBuffer or ndarray to WAV. Supports float32 or int16 PCM output.\"\"\"\n",
    "    if isinstance(buf, AudioBuffer):\n",
    "        y = buf.samples\n",
    "        sr_out = buf.sr\n",
    "    else:\n",
    "        if sr is None:\n",
    "            raise ValueError(\"When saving a raw ndarray, 'sr' must be provided.\")\n",
    "        y = np.asarray(buf, dtype=np.float32)\n",
    "        sr_out = sr\n",
    "\n",
    "    ensure_dir(path)\n",
    "    y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "    if bitdepth == \"float32\":\n",
    "        wavfile.write(path, sr_out, y.astype(np.float32))\n",
    "    elif bitdepth == \"int16\":\n",
    "        pcm16 = tpdf_dither_16bit(y) if dither_16bit else np.int16(np.round(np.clip(y, -1, 1) * 32767.0))\n",
    "        wavfile.write(path, sr_out, pcm16)\n",
    "    else:\n",
    "        raise ValueError(\"bitdepth must be 'float32' or 'int16'\")\n",
    "\n",
    "    return os.path.abspath(path)\n",
    "\n",
    "\n",
    "# ----------------------------- Convenience Summaries -----------------------------\n",
    "\n",
    "def print_audio_summary(buf: AudioBuffer, name: str = \"Audio\"):\n",
    "    s = buf.summary()\n",
    "    print(f\"{name}: sr={s['sr']} | ch={s['channels']} | dur={s['duration_s']}s | peak={s['peak']} | rms={s['rms']}\")\n",
    "    if buf.path:\n",
    "        print(f\"  path: {buf.path}\")\n",
    "    if buf.meta:\n",
    "        if \"source_sha256\" in buf.meta:\n",
    "            print(f\"  sha256: {buf.meta['source_sha256'][:16]}...\")\n",
    "        if \"dtype_in\" in buf.meta:\n",
    "            print(f\"  src dtype: {buf.meta['dtype_in']} | src ch: {buf.meta.get('channels_in')}\")\n",
    "\n",
    "\n",
    "def auto_out_path(input_path: str, stage: str, ext: str = \".wav\") -> str:\n",
    "    \"\"\"Create a standardized output filename like 'song__stage.wav' in same folder.\"\"\"\n",
    "    d, base = os.path.split(input_path)\n",
    "    stem, _ = os.path.splitext(base)\n",
    "    name = f\"{stem}__{stage}{ext}\"\n",
    "    return os.path.join(d, name)\n",
    "\n",
    "\n",
    "print(\"Post-Mix I/O layer loaded: AudioBuffer, load_wav, save_wav, resample_poly, slice_preview, with_suffix, auto_out_path, sha256_file, print_audio_summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80284394-53e6-4e73-ba87-ec6ba2ec5fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%writefile is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%writefile myscript.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f258282-5b65-4ca1-9ee6-522c39316c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convenience helpers loaded: make_workspace, batch_load_wavs, env_fingerprint, Manifest, write/read_manifest, register_input, register_artifact, import_mix.\n"
     ]
    }
   ],
   "source": [
    "# Post-Mix I/O — Convenience helpers for notebook workflows\n",
    "# Adds:\n",
    "# - batch_load_wavs (accepts glob or list)\n",
    "# - run workspace scaffolding (timestamped work dir tree)\n",
    "# - manifest creation/update (JSON)\n",
    "# - artifact registration (copy + hash + metadata)\n",
    "# - small utilities for copying, naming, and environment capture\n",
    "#\n",
    "# This cell *extends* the I/O layer previously loaded.\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Optional, Tuple, Dict, Any, List, Iterable, Union\n",
    "import os, glob, json, shutil, sys, platform\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# --- Workspace paths model ---\n",
    "@dataclass\n",
    "class RunPaths:\n",
    "    root: str\n",
    "    inputs: str\n",
    "    work: str\n",
    "    outputs: str\n",
    "    reports: str\n",
    "\n",
    "def _timestamp() -> str:\n",
    "    return datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def make_workspace(base_dir: str = \"./postmix_runs\", project: str = \"default\", slug: Optional[str] = None) -> RunPaths:\n",
    "    ts = _timestamp()\n",
    "    slug_part = f\"_{slug}\" if slug else \"\"\n",
    "    root = os.path.abspath(os.path.join(base_dir, f\"{project}_{ts}{slug_part}\"))\n",
    "    paths = RunPaths(\n",
    "        root=root,\n",
    "        inputs=os.path.join(root, \"inputs\"),\n",
    "        work=os.path.join(root, \"work\"),\n",
    "        outputs=os.path.join(root, \"outputs\"),\n",
    "        reports=os.path.join(root, \"reports\"),\n",
    "    )\n",
    "    for p in asdict(paths).values():\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "    print(f\"Workspace created at: {paths.root}\")\n",
    "    return paths\n",
    "\n",
    "# --- Batch load WAVs ---\n",
    "def batch_load_wavs(paths: Union[str, Iterable[str]], target_sr: Optional[int] = None, mono: bool = False) -> Dict[str, AudioBuffer]:\n",
    "    \"\"\"\n",
    "    Load multiple WAVs. 'paths' can be a glob pattern ('/path/*.wav') or an iterable of paths.\n",
    "    Returns dict: {stem: AudioBuffer}\n",
    "    \"\"\"\n",
    "    if isinstance(paths, str):\n",
    "        file_list = sorted(glob.glob(paths))\n",
    "    else:\n",
    "        file_list = list(paths)\n",
    "    if not file_list:\n",
    "        print(\"No files matched.\")\n",
    "        return {}\n",
    "\n",
    "    buffers: Dict[str, AudioBuffer] = {}\n",
    "    for p in file_list:\n",
    "        try:\n",
    "            buf = load_wav(p, target_sr=target_sr, mono=mono)\n",
    "            stem = os.path.splitext(os.path.basename(p))[0]\n",
    "            buffers[stem] = buf\n",
    "            print_audio_summary(buf, name=stem)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {p}: {e}\")\n",
    "    return buffers\n",
    "\n",
    "# --- Environment capture (for reproducibility) ---\n",
    "def env_fingerprint() -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"platform\": platform.platform(),\n",
    "        \"numpy\": np.__version__,\n",
    "        # scipy and others may be present from earlier cells\n",
    "        \"scipy\": __import__(\"scipy\").__version__ if \"scipy\" in sys.modules else None,\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "# --- Manifest model ---\n",
    "@dataclass\n",
    "class Manifest:\n",
    "    project: str\n",
    "    workspace: RunPaths\n",
    "    inputs: List[Dict[str, Any]] = field(default_factory=list)     # list of audio inputs and hashes\n",
    "    params: Dict[str, Any] = field(default_factory=dict)           # top-level run params (optional)\n",
    "    artifacts: List[Dict[str, Any]] = field(default_factory=list)  # produced files\n",
    "    env: Dict[str, Any] = field(default_factory=env_fingerprint)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        d = asdict(self)\n",
    "        # Expand dataclass paths as dict\n",
    "        d[\"workspace\"] = asdict(self.workspace)\n",
    "        return d\n",
    "\n",
    "def manifest_path(paths: RunPaths) -> str:\n",
    "    return os.path.join(paths.root, \"manifest.json\")\n",
    "\n",
    "def write_manifest(man: Manifest) -> str:\n",
    "    path = manifest_path(man.workspace)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(man.to_dict(), f, indent=2)\n",
    "    print(f\"Manifest written: {path}\")\n",
    "    return path\n",
    "\n",
    "def read_manifest(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- Artifact registration ---\n",
    "def copy_into(dst_dir: str, src_path: str, new_name: Optional[str] = None) -> str:\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    base = new_name if new_name else os.path.basename(src_path)\n",
    "    dst = os.path.join(dst_dir, base)\n",
    "    shutil.copy2(src_path, dst)\n",
    "    return os.path.abspath(dst)\n",
    "\n",
    "def register_input(man: Manifest, path: str, alias: Optional[str] = None) -> Dict[str, Any]:\n",
    "    info = {\n",
    "        \"alias\": alias or os.path.splitext(os.path.basename(path))[0],\n",
    "        \"path\": os.path.abspath(path),\n",
    "        \"sha256\": sha256_file(path),\n",
    "    }\n",
    "    man.inputs.append(info)\n",
    "    return info\n",
    "\n",
    "def register_artifact(man: Manifest, file_path: str, kind: str, params: Optional[Dict[str, Any]] = None, stage: Optional[str] = None) -> Dict[str, Any]:\n",
    "    rec = {\n",
    "        \"kind\": kind,                       # e.g., \"premaster\", \"master_landr\", \"stream_spotify\"\n",
    "        \"stage\": stage,                     # optional string label\n",
    "        \"path\": os.path.abspath(file_path),\n",
    "        \"sha256\": sha256_file(file_path),\n",
    "        \"params\": params or {},\n",
    "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "    man.artifacts.append(rec)\n",
    "    return rec\n",
    "\n",
    "# --- Convenience: bring source mix into workspace/inputs ---\n",
    "def import_mix(paths: RunPaths, source_path: str, alias: Optional[str] = None) -> str:\n",
    "    dst_name = (alias or os.path.basename(source_path))\n",
    "    dst = copy_into(paths.inputs, source_path, new_name=dst_name)\n",
    "    print(f\"Imported mix → {dst}\")\n",
    "    return dst\n",
    "\n",
    "print(\"Convenience helpers loaded: make_workspace, batch_load_wavs, env_fingerprint, Manifest, write/read_manifest, register_input, register_artifact, import_mix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0042f-2f2f-4115-a504-a4d77f8dd0cb",
   "metadata": {},
   "source": [
    "### Analysis layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058651bd-f240-49ce-a3dd-b9c1956e0ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis layer loaded: analyze_wav/analyze_audio_array, analysis_table, plot_spectrum, plot_short_term_loudness, plot_waveform_excerpt, LUFS approx, true-peak approx, stereo & health metrics.\n"
     ]
    }
   ],
   "source": [
    "# Analysis Layer — Notebook Version\n",
    "# Robust audio analysis utilities for post-mix:\n",
    "# - Health checks (DC, peak/RMS, true-peak approx, headroom, NaN/Inf)\n",
    "# - Loudness (K-weighted momentary/short-term + approx integrated LUFS)\n",
    "# - Dynamics proxies (crest, short-term distribution, DR proxy)\n",
    "# - Spectrum & band energy (bass/air %), spectral flatness\n",
    "# - Stereo metrics (phase correlation, width proxy, mid/side peaks)\n",
    "# - Plots: spectrum, short-term loudness, waveform excerpt (matplotlib; no seaborn)\n",
    "#\n",
    "# Designed to integrate with the earlier I/O layer (AudioBuffer, load_wav).\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "except Exception:\n",
    "    display_dataframe_to_user = None\n",
    "\n",
    "\n",
    "# ----------------------------- Utilities -----------------------------\n",
    "\n",
    "def _lin_to_db(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    return 20.0 * np.log10(np.maximum(eps, np.abs(x)))\n",
    "\n",
    "def _db_to_lin(db: float) -> float:\n",
    "    return 10.0 ** (db / 20.0)\n",
    "\n",
    "def _ensure_stereo(x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim == 1:\n",
    "        return np.stack([x, x], axis=-1)\n",
    "    if x.shape[1] == 1:\n",
    "        return np.repeat(x, 2, axis=1)\n",
    "    return x\n",
    "\n",
    "def _mono(x: np.ndarray) -> np.ndarray:\n",
    "    return x if x.ndim == 1 else np.mean(x, axis=1)\n",
    "\n",
    "def _sanitize(x: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0), -1.0, 1.0)\n",
    "\n",
    "\n",
    "# ----------------------------- True Peak (approx) -----------------------------\n",
    "\n",
    "def true_peak_dbfs(x: np.ndarray, sr: int, oversample: int = 4) -> float:\n",
    "    \"\"\"\n",
    "    Approximate true peak by oversampling with polyphase resampling and taking the max.\n",
    "    \"\"\"\n",
    "    x = _sanitize(x)\n",
    "    x_os = signal.resample_poly(x, oversample, 1, axis=0 if x.ndim > 1 else 0)\n",
    "    tp = float(np.max(np.abs(x_os)))\n",
    "    return float(_lin_to_db(np.array([tp]))[0])\n",
    "\n",
    "\n",
    "# ----------------------------- K-Weighting (BS.1770-style) -----------------------------\n",
    "\n",
    "def _k_weighting_sos(sr: int):\n",
    "    \"\"\"\n",
    "    Return SOS for the K-weighting pre-filter + high-frequency shelf per ITU-R BS.1770.\n",
    "    Using standard bilinear transforms for the defined z-plane filters.\n",
    "    \"\"\"\n",
    "    # High-pass (2nd order) at 38 Hz (pre-filter)\n",
    "    f_hp = 38.0\n",
    "    # High-shelf (2nd order) with +4 dB above ~1 kHz\n",
    "    f_shelf = 1681.974450955533\n",
    "    Q_shelf = 0.7071752369554196\n",
    "    gain_db = 3.99984385397  # ~+4 dB\n",
    "\n",
    "    # HPF\n",
    "    sos_hp = signal.butter(2, f_hp/(sr*0.5), btype='highpass', output='sos')\n",
    "\n",
    "    # High-shelf (RBJ biquad in SOS form)\n",
    "    A = 10**(gain_db/40.0)\n",
    "    w0 = 2*np.pi*f_shelf/sr\n",
    "    alpha = np.sin(w0)/(2*Q_shelf)\n",
    "    cosw0 = np.cos(w0)\n",
    "    b0 =    A*((A+1) + (A-1)*cosw0 + 2*np.sqrt(A)*alpha)\n",
    "    b1 = -2*A*((A-1) + (A+1)*cosw0)\n",
    "    b2 =    A*((A+1) + (A-1)*cosw0 - 2*np.sqrt(A)*alpha)\n",
    "    a0 =        (A+1) - (A-1)*cosw0 + 2*np.sqrt(A)*alpha\n",
    "    a1 =    2*((A-1) - (A+1)*cosw0)\n",
    "    a2 =        (A+1) - (A-1)*cosw0 - 2*np.sqrt(A)*alpha\n",
    "    sos_shelf = signal.tf2sos([b0/a0, b1/a0, b2/a0], [1.0, a1/a0, a2/a0])\n",
    "\n",
    "    return np.vstack([sos_hp, sos_shelf])\n",
    "\n",
    "def k_weight(x: np.ndarray, sr: int) -> np.ndarray:\n",
    "    \"\"\"Apply K-weighting to mono signal.\"\"\"\n",
    "    sos = _k_weighting_sos(sr)\n",
    "    return signal.sosfilt(sos, x)\n",
    "\n",
    "def lufs_momentary(x_mono: np.ndarray, sr: int, window_s: float = 0.4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    400 ms momentary LUFS approximation (BS.1770 weighting, no gating).\n",
    "    Returns (time_axis, lufs_array).\n",
    "    \"\"\"\n",
    "    xk = k_weight(x_mono, sr)\n",
    "    win = int(max(1, round(window_s * sr)))\n",
    "    # mean square via moving average\n",
    "    kernel = np.ones(win, dtype=np.float32) / float(win)\n",
    "    ms = np.convolve(xk**2, kernel, mode='same')\n",
    "    lufs = -0.691 + 10.0 * np.log10(np.maximum(1e-12, ms))  # -0.691 is the BS.1770 absolute scale offset\n",
    "    t = np.arange(len(lufs)) / sr\n",
    "    return t, lufs\n",
    "\n",
    "def lufs_integrated_approx(x_mono: np.ndarray, sr: int) -> float:\n",
    "    \"\"\"\n",
    "    Very lightweight integrated LUFS approximation (K-weighted, no gating).\n",
    "    For streaming normalization preview, this is often sufficient.\n",
    "    \"\"\"\n",
    "    xk = k_weight(x_mono, sr)\n",
    "    ms = np.mean(xk**2)\n",
    "    lufs = -0.691 + 10.0 * np.log10(np.maximum(1e-12, ms))\n",
    "    return float(lufs)\n",
    "\n",
    "\n",
    "# ----------------------------- Spectrum & Bands -----------------------------\n",
    "\n",
    "def spectrum_db(mono: np.ndarray, sr: int, n_fft: int = 1<<16) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    seg = mono[:min(len(mono), n_fft)]\n",
    "    if len(seg) < 2048:\n",
    "        pad = np.zeros(2048, dtype=seg.dtype)\n",
    "        pad[:len(seg)] = seg\n",
    "        seg = pad\n",
    "    win = np.hanning(len(seg))\n",
    "    sp = np.fft.rfft(seg * win)\n",
    "    freqs = np.fft.rfftfreq(len(seg), 1/sr)\n",
    "    mag_db = _lin_to_db(np.abs(sp))\n",
    "    return freqs, mag_db\n",
    "\n",
    "def band_energy_percent(mono: np.ndarray, sr: int, f_lo: float, f_hi: float) -> float:\n",
    "    n = 1<<16\n",
    "    seg = mono[:min(len(mono), n)]\n",
    "    win = np.hanning(len(seg))\n",
    "    sp = np.fft.rfft(seg * win)\n",
    "    freqs = np.fft.rfftfreq(len(seg), 1/sr)\n",
    "    power = (np.abs(sp)**2)\n",
    "    total = np.sum(power) + 1e-20\n",
    "    mask = (freqs >= f_lo) & (freqs < f_hi)\n",
    "    band = np.sum(power[mask])\n",
    "    return float(100.0 * band / total)\n",
    "\n",
    "def spectral_flatness(mono: np.ndarray, sr: int) -> float:\n",
    "    n = 1<<14\n",
    "    seg = mono[:min(len(mono), n)]\n",
    "    win = np.hanning(len(seg))\n",
    "    sp = np.abs(np.fft.rfft(seg * win)) + 1e-12\n",
    "    geo = np.exp(np.mean(np.log(sp)))\n",
    "    ari = np.mean(sp)\n",
    "    return float(np.clip(geo / ari, 0.0, 1.0))\n",
    "\n",
    "\n",
    "# ----------------------------- Stereo Metrics -----------------------------\n",
    "\n",
    "def stereo_metrics(x: np.ndarray) -> Dict[str, float]:\n",
    "    x = _ensure_stereo(x)\n",
    "    L = x[:, 0]; R = x[:, 1]\n",
    "    # Phase correlation: mean of normalized instantaneous product\n",
    "    denom = np.maximum(1e-12, np.sqrt(L**2) * np.sqrt(R**2))\n",
    "    corr = float(np.mean((L * R) / denom))\n",
    "    # Width proxy using mid/side\n",
    "    M = 0.5 * (L + R); S = 0.5 * (L - R)\n",
    "    width = float(np.mean(np.abs(S)) / (np.mean(np.abs(M)) + 1e-12))\n",
    "    return {\n",
    "        \"phase_correlation\": float(np.clip(corr, -1.0, 1.0)),\n",
    "        \"stereo_width\": width,\n",
    "        \"mid_peak_db\": float(_lin_to_db(np.array([np.max(np.abs(M))]))[0]),\n",
    "        \"side_peak_db\": float(_lin_to_db(np.array([np.max(np.abs(S))]))[0]),\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------- Health & Dynamics -----------------------------\n",
    "\n",
    "def health_metrics(x: np.ndarray, sr: int) -> Dict[str, float]:\n",
    "    mono = _mono(x)\n",
    "    dc = float(np.mean(mono))\n",
    "    dc_db = float(_lin_to_db(np.array([abs(dc) if abs(dc) > 0 else 1e-12]))[0])\n",
    "    peak = float(np.max(np.abs(mono)))\n",
    "    peak_db = float(_lin_to_db(np.array([peak]))[0])\n",
    "    rms = float(np.sqrt(np.mean(mono**2)))\n",
    "    rms_db = float(_lin_to_db(np.array([rms]))[0])\n",
    "    crest = peak_db - rms_db\n",
    "    sub_pct = band_energy_percent(mono, sr, 0.0, 30.0)\n",
    "    return {\n",
    "        \"peak_dbfs\": peak_db,\n",
    "        \"rms_dbfs\": rms_db,\n",
    "        \"crest_db\": crest,\n",
    "        \"dc_offset\": dc,\n",
    "        \"dc_dbfs\": dc_db,\n",
    "        \"sub_30Hz_%\": sub_pct,\n",
    "    }\n",
    "\n",
    "def short_term_loudness(mono: np.ndarray, sr: int, win_s: float = 3.0, hop_s: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    win = int(max(2, round(win_s * sr)))\n",
    "    hop = int(max(1, round(hop_s * sr)))\n",
    "    kernel = np.ones(win, dtype=np.float32) / float(win)\n",
    "    pow_sig = mono**2\n",
    "    rms = np.sqrt(np.maximum(1e-20, np.convolve(pow_sig, kernel, mode=\"same\")))\n",
    "    idx = np.arange(0, len(mono), hop)\n",
    "    return idx / sr, _lin_to_db(rms[idx])\n",
    "\n",
    "def dr_proxy(mono: np.ndarray, sr: int) -> float:\n",
    "    t, st = short_term_loudness(mono, sr, win_s=3.0, hop_s=0.5)\n",
    "    return float(np.percentile(st, 95) - np.percentile(st, 10))\n",
    "\n",
    "\n",
    "# ----------------------------- Analyzer Facade -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class AnalysisReport:\n",
    "    sr: int\n",
    "    duration_s: float\n",
    "    basic: Dict[str, float]\n",
    "    stereo: Dict[str, float]\n",
    "    lufs_integrated: float\n",
    "    true_peak_dbfs: float\n",
    "    bass_energy_pct: float\n",
    "    air_energy_pct: float\n",
    "    spectral_flatness: float\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        rows.append({\"metric\": \"sr\", \"value\": self.sr})\n",
    "        rows.append({\"metric\": \"duration_s\", \"value\": self.duration_s})\n",
    "        for k, v in self.basic.items():\n",
    "            rows.append({\"metric\": k, \"value\": v})\n",
    "        for k, v in self.stereo.items():\n",
    "            rows.append({\"metric\": k, \"value\": v})\n",
    "        rows.append({\"metric\": \"lufs_integrated\", \"value\": self.lufs_integrated})\n",
    "        rows.append({\"metric\": \"true_peak_dbfs\", \"value\": self.true_peak_dbfs})\n",
    "        rows.append({\"metric\": \"bass_energy_%\", \"value\": self.bass_energy_pct})\n",
    "        rows.append({\"metric\": \"air_energy_%\", \"value\": self.air_energy_pct})\n",
    "        rows.append({\"metric\": \"spectral_flatness\", \"value\": self.spectral_flatness})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "def analyze_audio_array(x: np.ndarray, sr: int) -> AnalysisReport:\n",
    "    x = _sanitize(x)\n",
    "    x = _ensure_stereo(x)\n",
    "    mono = _mono(x)\n",
    "    dur_s = len(mono) / sr\n",
    "\n",
    "    basic = health_metrics(x, sr)\n",
    "    stereo = stereo_metrics(x)\n",
    "    tp = true_peak_dbfs(x, sr, oversample=4)\n",
    "    bass_pct = band_energy_percent(mono, sr, 20.0, 120.0)\n",
    "    air_pct = band_energy_percent(mono, sr, 8000.0, sr/2)\n",
    "    flat = spectral_flatness(mono, sr)\n",
    "    lufs_i = lufs_integrated_approx(mono, sr)\n",
    "\n",
    "    return AnalysisReport(\n",
    "        sr=sr,\n",
    "        duration_s=dur_s,\n",
    "        basic=basic,\n",
    "        stereo=stereo,\n",
    "        lufs_integrated=lufs_i,\n",
    "        true_peak_dbfs=tp,\n",
    "        bass_energy_pct=bass_pct,\n",
    "        air_energy_pct=air_pct,\n",
    "        spectral_flatness=flat,\n",
    "    )\n",
    "\n",
    "def analyze_wav(path: str, target_sr: Optional[int] = None) -> AnalysisReport:\n",
    "    sr, data = wavfile.read(path)\n",
    "    x = data.astype(np.float32)\n",
    "    if data.dtype == np.int16:\n",
    "        x = data.astype(np.float32) / 32768.0\n",
    "    elif data.dtype == np.int32:\n",
    "        x = data.astype(np.float32) / 2147483648.0\n",
    "    elif data.dtype == np.uint8:\n",
    "        x = (data.astype(np.float32) - 128.0) / 128.0\n",
    "    # resample if requested\n",
    "    if target_sr and target_sr != sr:\n",
    "        gcd = np.gcd(sr, target_sr)\n",
    "        x = signal.resample_poly(x, target_sr//gcd, sr//gcd, axis=0 if x.ndim > 1 else 0)\n",
    "        sr = target_sr\n",
    "    return analyze_audio_array(x, sr)\n",
    "\n",
    "\n",
    "# ----------------------------- Plot Helpers -----------------------------\n",
    "\n",
    "def plot_spectrum(path_or_array, sr: Optional[int] = None, fmax: float = 20000.0):\n",
    "    if isinstance(path_or_array, str):\n",
    "        rep = analyze_wav(path_or_array)\n",
    "        sr0, data = wavfile.read(path_or_array)\n",
    "        x = data.astype(np.float32)\n",
    "        if data.dtype == np.int16:\n",
    "            x = data.astype(np.float32) / 32768.0\n",
    "        elif data.dtype == np.int32:\n",
    "            x = data.astype(np.float32) / 2147483648.0\n",
    "        elif data.dtype == np.uint8:\n",
    "            x = (data.astype(np.float32) - 128.0) / 128.0\n",
    "        mono = _mono(x)\n",
    "        freqs, mag_db = spectrum_db(mono, sr0)\n",
    "    else:\n",
    "        x = path_or_array\n",
    "        assert sr is not None, \"When passing an array, provide sr.\"\n",
    "        mono = _mono(x)\n",
    "        freqs, mag_db = spectrum_db(mono, sr)\n",
    "        rep = None\n",
    "\n",
    "    mask = freqs <= fmax\n",
    "    plt.figure()\n",
    "    plt.plot(freqs[mask], mag_db[mask])\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.title(\"Magnitude Spectrum\")\n",
    "    plt.show()\n",
    "    return rep\n",
    "\n",
    "def plot_short_term_loudness(path_or_array, sr: Optional[int] = None, win_s: float = 3.0, hop_s: float = 0.5):\n",
    "    if isinstance(path_or_array, str):\n",
    "        sr0, data = wavfile.read(path_or_array)\n",
    "        x = data.astype(np.float32)\n",
    "        if data.dtype == np.int16:\n",
    "            x = data.astype(np.float32) / 32768.0\n",
    "        elif data.dtype == np.int32:\n",
    "            x = data.astype(np.float32) / 2147483648.0\n",
    "        elif data.dtype == np.uint8:\n",
    "            x = (data.astype(np.float32) - 128.0) / 128.0\n",
    "        mono = _mono(x)\n",
    "        t, st = short_term_loudness(mono, sr0, win_s=win_s, hop_s=hop_s)\n",
    "    else:\n",
    "        x = path_or_array\n",
    "        assert sr is not None, \"When passing an array, provide sr.\"\n",
    "        mono = _mono(x)\n",
    "        t, st = short_term_loudness(mono, sr, win_s=win_s, hop_s=hop_s)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t, st)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Short-term RMS (dBFS)\")\n",
    "    plt.title(\"Short-term Loudness\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_waveform_excerpt(path_or_array, sr: Optional[int] = None, start_s: float = 0.0, dur_s: float = 10.0):\n",
    "    if isinstance(path_or_array, str):\n",
    "        sr0, data = wavfile.read(path_or_array)\n",
    "        x = data.astype(np.float32)\n",
    "        if data.dtype == np.int16:\n",
    "            x = data.astype(np.float32) / 32768.0\n",
    "        elif data.dtype == np.int32:\n",
    "            x = data.astype(np.float32) / 2147483648.0\n",
    "        elif data.dtype == np.uint8:\n",
    "            x = (data.astype(np.float32) - 128.0) / 128.0\n",
    "        sr = sr0\n",
    "    else:\n",
    "        x = path_or_array\n",
    "        assert sr is not None, \"When passing an array, provide sr.\"\n",
    "\n",
    "    x = _ensure_stereo(x)\n",
    "    n0 = int(start_s * sr); n1 = int((start_s + dur_s) * sr)\n",
    "    n1 = min(n1, x.shape[0])\n",
    "    t = np.arange(n0, n1) / sr\n",
    "    mono = _mono(x[n0:n1, :]) if x.ndim > 1 else _mono(x[n0:n1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t, mono)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (mono)\")\n",
    "    plt.title(f\"Waveform Excerpt ({start_s:.1f}s–{start_s+dur_s:.1f}s)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------- Tabular Summary -----------------------------\n",
    "\n",
    "def analysis_table(report: AnalysisReport, name: str = \"Track Analysis\") -> pd.DataFrame:\n",
    "    df = report.to_dataframe()\n",
    "    if display_dataframe_to_user:\n",
    "        display_dataframe_to_user(name, df)\n",
    "    else:\n",
    "        print(df.to_string(index=False))\n",
    "    return df\n",
    "\n",
    "print(\"Analysis layer loaded: analyze_wav/analyze_audio_array, analysis_table, plot_spectrum, plot_short_term_loudness, plot_waveform_excerpt, LUFS approx, true-peak approx, stereo & health metrics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80f55e-dd68-434a-b632-6677f9a126c0",
   "metadata": {},
   "source": [
    "### DSP Primitives Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8d0ab8-9490-473c-9a3d-8db1db41617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSP Primitives Layer loaded:\n",
      "- Gain/level: apply_gain_db, normalize_peak, normalize_lufs, measure_peak, measure_rms\n",
      "- Filters: highpass_filter, lowpass_filter, bandpass_filter, shelf_filter, peaking_eq, notch_filter, tilt_eq\n",
      "- Stereo: mid_side_encode, mid_side_decode, stereo_widener\n",
      "- Dynamics: compressor (soft‑knee), transient_shaper\n",
      "- Fades: fade_in, fade_out\n",
      "- K‑weighting/LUFS approx: k_weight, lufs_integrated_approx\n"
     ]
    }
   ],
   "source": [
    "# DSP Primitives Layer — Notebook Implementation\n",
    "# Reusable, low-level DSP blocks for building post‑mix features.\n",
    "# Dependencies: numpy, scipy.signal\n",
    "#\n",
    "# All functions accept/return numpy arrays:\n",
    "# - audio: shape (N,) mono or (N,2) stereo\n",
    "# - sr: sample rate (int)\n",
    "#\n",
    "# Notes:\n",
    "# - Uses numerically-stable SOS filters where applicable\n",
    "# - Sanitizes NaN/Inf and clamps frequencies to safe ranges\n",
    "# - Stereo‑aware (processes both channels consistently)\n",
    "#\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from typing import Tuple\n",
    "\n",
    "# --------- Core helpers ---------\n",
    "\n",
    "def _sanitize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Replace NaN/Inf and clamp extreme outliers to avoid IIR blowups.\"\"\"\n",
    "    return np.clip(np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0), -4.0, 4.0).astype(np.float32)\n",
    "\n",
    "def _ensure_stereo(x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim == 1:\n",
    "        return np.stack([x, x], axis=-1)\n",
    "    if x.shape[1] == 1:\n",
    "        return np.repeat(x, 2, axis=1)\n",
    "    return x\n",
    "\n",
    "def _mono(x: np.ndarray) -> np.ndarray:\n",
    "    return x if x.ndim == 1 else np.mean(x, axis=1)\n",
    "\n",
    "def _safe_freq(f: float, sr: int, lo: float = 10.0, hi_ratio: float = 0.49) -> float:\n",
    "    \"\"\"Clamp frequency to a safe absolute Hz range based on sample rate.\"\"\"\n",
    "    return float(max(lo, min(f, hi_ratio * sr)))\n",
    "\n",
    "def _db_to_lin(db: float) -> float:\n",
    "    return 10.0 ** (db / 20.0)\n",
    "\n",
    "def _lin_to_db(lin: float, eps: float = 1e-12) -> float:\n",
    "    return 20.0 * np.log10(max(eps, abs(lin)))\n",
    "\n",
    "# --------- Gain & leveling ---------\n",
    "\n",
    "def apply_gain_db(audio: np.ndarray, db: float) -> np.ndarray:\n",
    "    \"\"\"Apply linear gain in dB (stereo‑safe).\"\"\"\n",
    "    g = _db_to_lin(db)\n",
    "    return (_sanitize(audio) * g).astype(np.float32)\n",
    "\n",
    "def measure_peak(audio: np.ndarray) -> float:\n",
    "    \"\"\"Return peak linear amplitude (mono‑collapsed).\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    x = _mono(x)\n",
    "    return float(np.max(np.abs(x)))\n",
    "\n",
    "def measure_rms(audio: np.ndarray) -> float:\n",
    "    \"\"\"Return RMS (linear) on mono‑collapsed signal.\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    x = _mono(x)\n",
    "    return float(np.sqrt(np.mean(x**2)))\n",
    "\n",
    "def normalize_peak(audio: np.ndarray, target_dbfs: float = -1.0, eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"Scale so that the absolute peak ≈ target dBFS.\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    peak = float(np.max(np.abs(x)))\n",
    "    if peak < eps:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    g = _db_to_lin(target_dbfs) / peak\n",
    "    return (x * g).astype(np.float32)\n",
    "\n",
    "# --------- K‑weighting & LUFS (approx) ---------\n",
    "\n",
    "def _k_weighting_sos(sr: int) -> np.ndarray:\n",
    "    \"\"\"SOS for ITU‑R BS.1770 K‑weighting: 2nd‑order HPF (~38 Hz) + 2nd‑order high‑shelf (~+4 dB @ 1 kHz).\"\"\"\n",
    "    f_hp = 38.0\n",
    "    sos_hp = signal.butter(2, _safe_freq(f_hp, sr)/(sr*0.5), btype='highpass', output='sos')\n",
    "    # High‑shelf (RBJ)\n",
    "    f_shelf = 1681.974450955533\n",
    "    Q_shelf = 0.7071752369554196\n",
    "    gain_db = 3.99984385397\n",
    "    A = 10**(gain_db/40.0)\n",
    "    w0 = 2*np.pi*f_shelf/sr\n",
    "    alpha = np.sin(w0)/(2*Q_shelf)\n",
    "    cosw0 = np.cos(w0)\n",
    "    b0 =    A*((A+1) + (A-1)*cosw0 + 2*np.sqrt(A)*alpha)\n",
    "    b1 = -2*A*((A-1) + (A+1)*cosw0)\n",
    "    b2 =    A*((A+1) + (A-1)*cosw0 - 2*np.sqrt(A)*alpha)\n",
    "    a0 =        (A+1) - (A-1)*cosw0 + 2*np.sqrt(A)*alpha\n",
    "    a1 =    2*((A-1) - (A+1)*cosw0)\n",
    "    a2 =        (A+1) - (A-1)*cosw0 - 2*np.sqrt(A)*alpha\n",
    "    sos_shelf = signal.tf2sos([b0/a0, b1/a0, b2/a0], [1.0, a1/a0, a2/a0])\n",
    "    return np.vstack([sos_hp, sos_shelf])\n",
    "\n",
    "def k_weight(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    \"\"\"Apply K‑weighting to mono signal (array).\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    x = _mono(x)\n",
    "    sos = _k_weighting_sos(sr)\n",
    "    return signal.sosfilt(sos, x).astype(np.float32)\n",
    "\n",
    "def lufs_integrated_approx(audio: np.ndarray, sr: int) -> float:\n",
    "    \"\"\"Lightweight integrated LUFS (BS.1770 K‑weighting, no gating).\"\"\"\n",
    "    xk = k_weight(audio, sr)\n",
    "    ms = float(np.mean(xk**2))\n",
    "    return -0.691 + 10.0 * np.log10(max(1e-12, ms))\n",
    "\n",
    "def normalize_lufs(audio: np.ndarray, sr: int, target_lufs: float = -14.0) -> np.ndarray:\n",
    "    \"\"\"Normalize integrated loudness to target LUFS (approximate, no gating).\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    current = lufs_integrated_approx(x, sr)\n",
    "    delta = target_lufs - current  # dB to add\n",
    "    return apply_gain_db(x, delta)\n",
    "\n",
    "# --------- Filtering (SOS) ---------\n",
    "\n",
    "def highpass_filter(audio: np.ndarray, sr: int, cutoff_hz: float, order: int = 4) -> np.ndarray:\n",
    "    x = _sanitize(audio)\n",
    "    sos = signal.butter(order, _safe_freq(cutoff_hz, sr)/(sr*0.5), btype='highpass', output='sos')\n",
    "    return signal.sosfilt(sos, x, axis=0 if x.ndim > 1 else 0).astype(np.float32)\n",
    "\n",
    "def lowpass_filter(audio: np.ndarray, sr: int, cutoff_hz: float, order: int = 4) -> np.ndarray:\n",
    "    x = _sanitize(audio)\n",
    "    sos = signal.butter(order, _safe_freq(cutoff_hz, sr)/(sr*0.5), btype='lowpass', output='sos')\n",
    "    return signal.sosfilt(sos, x, axis=0 if x.ndim > 1 else 0).astype(np.float32)\n",
    "\n",
    "def bandpass_filter(audio: np.ndarray, sr: int, f_lo: float, f_hi: float, order: int = 4) -> np.ndarray:\n",
    "    x = _sanitize(audio)\n",
    "    lo = _safe_freq(f_lo, sr)\n",
    "    hi = _safe_freq(f_hi, sr)\n",
    "    if hi <= lo:  # enforce valid band\n",
    "        hi = min(max(lo * 1.2, lo + 5.0), 0.49 * sr)\n",
    "    sos = signal.butter(order, [lo/(sr*0.5), hi/(sr*0.5)], btype='bandpass', output='sos')\n",
    "    return signal.sosfilt(sos, x, axis=0 if x.ndim > 1 else 0).astype(np.float32)\n",
    "\n",
    "# --------- Shelving & Parametric EQ (RBJ biquads -> SOS) ---------\n",
    "\n",
    "def _biquad_peaking_sos(sr: int, f0: float, gain_db: float, Q: float = 0.707) -> np.ndarray:\n",
    "    A = 10**(gain_db/40.0)\n",
    "    w0 = 2*np.pi*_safe_freq(f0, sr)/sr\n",
    "    alpha = np.sin(w0)/(2*Q)\n",
    "    cosw0 = np.cos(w0)\n",
    "    b0 = 1 + alpha*A\n",
    "    b1 = -2*cosw0\n",
    "    b2 = 1 - alpha*A\n",
    "    a0 = 1 + alpha/A\n",
    "    a1 = -2*cosw0\n",
    "    a2 = 1 - alpha/A\n",
    "    b = np.array([b0, b1, b2])/a0\n",
    "    a = np.array([1.0, a1/a0, a2/a0])\n",
    "    return signal.tf2sos(b, a)\n",
    "\n",
    "def _biquad_lowshelf_sos(sr: int, f0: float, gain_db: float, S: float = 0.5) -> np.ndarray:\n",
    "    A = 10**(gain_db/40.0)\n",
    "    w0 = 2*np.pi*_safe_freq(f0, sr)/sr\n",
    "    cosw0 = np.cos(w0); sinw0 = np.sin(w0)\n",
    "    alpha = sinw0/2 * np.sqrt((A + 1/A)*(1/S - 1) + 2)\n",
    "    b0 =    A*((A+1) - (A-1)*cosw0 + 2*np.sqrt(A)*alpha)\n",
    "    b1 =  2*A*((A-1) - (A+1)*cosw0)\n",
    "    b2 =    A*((A+1) - (A-1)*cosw0 - 2*np.sqrt(A)*alpha)\n",
    "    a0 =        (A+1) + (A-1)*cosw0 + 2*np.sqrt(A)*alpha\n",
    "    a1 =   -2*((A-1) + (A+1)*cosw0)\n",
    "    a2 =        (A+1) + (A-1)*cosw0 - 2*np.sqrt(A)*alpha\n",
    "    b = np.array([b0, b1, b2])/a0\n",
    "    a = np.array([1.0, a1/a0, a2/a0])\n",
    "    return signal.tf2sos(b, a)\n",
    "\n",
    "def _biquad_highshelf_sos(sr: int, f0: float, gain_db: float, S: float = 0.5) -> np.ndarray:\n",
    "    A = 10**(gain_db/40.0)\n",
    "    w0 = 2*np.pi*_safe_freq(f0, sr)/sr\n",
    "    cosw0 = np.cos(w0); sinw0 = np.sin(w0)\n",
    "    alpha = sinw0/2 * np.sqrt((A + 1/A)*(1/S - 1) + 2)\n",
    "    b0 =    A*((A+1) + (A-1)*cosw0 + 2*np.sqrt(A)*alpha)\n",
    "    b1 = -2*A*((A-1) + (A+1)*cosw0)\n",
    "    b2 =    A*((A+1) + (A-1)*cosw0 - 2*np.sqrt(A)*alpha)\n",
    "    a0 =        (A+1) - (A-1)*cosw0 + 2*np.sqrt(A)*alpha\n",
    "    a1 =    2*((A-1) - (A+1)*cosw0)\n",
    "    a2 =        (A+1) - (A-1)*cosw0 - 2*np.sqrt(A)*alpha\n",
    "    b = np.array([b0, b1, b2])/a0\n",
    "    a = np.array([1.0, a1/a0, a2/a0])\n",
    "    return signal.tf2sos(b, a)\n",
    "\n",
    "def peaking_eq(audio: np.ndarray, sr: int, f0: float, gain_db: float, Q: float = 0.707) -> np.ndarray:\n",
    "    \"\"\"Parametric peaking EQ at f0 with gain_db and quality Q.\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    sos = _biquad_peaking_sos(sr, f0, gain_db, Q)\n",
    "    return signal.sosfilt(sos, x, axis=0 if x.ndim > 1 else 0).astype(np.float32)\n",
    "\n",
    "def shelf_filter(audio: np.ndarray, sr: int, cutoff_hz: float, gain_db: float, kind: str = \"low\", S: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Low/High shelf EQ using RBJ biquad. kind ∈ {'low','high'}\"\"\"\n",
    "    x = _sanitize(audio)\n",
    "    if kind == \"low\":\n",
    "        sos = _biquad_lowshelf_sos(sr, cutoff_hz, gain_db, S)\n",
    "    elif kind == \"high\":\n",
    "        sos = _biquad_highshelf_sos(sr, cutoff_hz, gain_db, S)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'low' or 'high'\")\n",
    "    return signal.sosfilt(sos, x, axis=0 if x.ndim > 1 else 0).astype(np.float32)\n",
    "\n",
    "def notch_filter(audio: np.ndarray, sr: int, f0: float, Q: float = 10.0) -> np.ndarray:\n",
    "    \"\"\"Narrow notch (peaking with large negative gain).\"\"\"\n",
    "    # Implement via iirnotch for convenience\n",
    "    w0 = _safe_freq(f0, sr)/(sr*0.5)\n",
    "    b, a = signal.iirnotch(w0, Q)\n",
    "    sos = signal.tf2sos(b, a)\n",
    "    return signal.sosfilt(sos, _sanitize(audio), axis=0 if audio.ndim > 1 else 0).astype(np.float32)\n",
    "\n",
    "def tilt_eq(audio: np.ndarray, sr: int, pivot_hz: float = 1000.0, gain_db: float = 1.5) -> np.ndarray:\n",
    "    \"\"\"Simple 'tilt' EQ via two wide peaks (approximate): low cut + high lift around pivot.\"\"\"\n",
    "    x = peaking_eq(audio, sr, f0=max(80.0, pivot_hz/5), gain_db=-gain_db/2, Q=0.7)\n",
    "    x = peaking_eq(x, sr, f0=min(sr*0.45, pivot_hz*5), gain_db=+gain_db/2, Q=0.7)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "# --------- Mid/Side & Stereo ---------\n",
    "\n",
    "def mid_side_encode(audio: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (M, S) mid/side from stereo input.\"\"\"\n",
    "    x = _ensure_stereo(_sanitize(audio))\n",
    "    L, R = x[:,0], x[:,1]\n",
    "    M = 0.5 * (L + R)\n",
    "    S = 0.5 * (L - R)\n",
    "    return M.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "def mid_side_decode(M: np.ndarray, S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return stereo from (M, S).\"\"\"\n",
    "    L = M + S\n",
    "    R = M - S\n",
    "    return np.column_stack([L, R]).astype(np.float32)\n",
    "\n",
    "def stereo_widener(audio: np.ndarray, width: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adjust stereo width by scaling the side channel.\n",
    "    width = 1.0 unchanged; >1 wider; <1 narrower.\n",
    "    \"\"\"\n",
    "    x = _ensure_stereo(_sanitize(audio))\n",
    "    M, S = mid_side_encode(x)\n",
    "    y = mid_side_decode(M, S * float(width))\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "# --------- Dynamics ---------\n",
    "\n",
    "def _envelope_detector(mono: np.ndarray, sr: int, attack_ms: float = 10.0, release_ms: float = 100.0) -> np.ndarray:\n",
    "    a_a = np.exp(-1.0 / ((attack_ms/1000.0) * sr))\n",
    "    a_r = np.exp(-1.0 / ((release_ms/1000.0) * sr))\n",
    "    env = np.zeros_like(mono, dtype=np.float32)\n",
    "    prev = 0.0\n",
    "    for i, v in enumerate(np.abs(mono)):\n",
    "        if v > prev:\n",
    "            prev = a_a*prev + (1 - a_a)*v\n",
    "        else:\n",
    "            prev = a_r*prev + (1 - a_r)*v\n",
    "        env[i] = prev\n",
    "    return env\n",
    "\n",
    "def compressor(audio: np.ndarray, sr: int,\n",
    "               threshold_db: float = -18.0, ratio: float = 2.0,\n",
    "               attack_ms: float = 15.0, release_ms: float = 120.0,\n",
    "               makeup_db: float = 0.0, knee_db: float = 3.0,\n",
    "               link_stereo: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Feed‑forward compressor with soft knee. If link_stereo=True, uses shared gain for both channels.\n",
    "    \"\"\"\n",
    "    x = _ensure_stereo(_sanitize(audio))\n",
    "    M = _mono(x) if link_stereo else None\n",
    "    if link_stereo:\n",
    "        env = _envelope_detector(M, sr, attack_ms, release_ms)\n",
    "    else:\n",
    "        envL = _envelope_detector(x[:,0], sr, attack_ms, release_ms)\n",
    "        envR = _envelope_detector(x[:,1], sr, attack_ms, release_ms)\n",
    "\n",
    "    thr = _db_to_lin(threshold_db)\n",
    "    knee = _db_to_lin(max(0.0, knee_db))\n",
    "\n",
    "    def gain_curve(env_val: float) -> float:\n",
    "        e = env_val\n",
    "        if e <= thr / knee:\n",
    "            g = 1.0\n",
    "        elif e <= thr * knee:\n",
    "            edb = _lin_to_db(e)\n",
    "            over = max(0.0, edb - threshold_db)\n",
    "            comp_db = over - (over / ratio)\n",
    "            g = _db_to_lin(-comp_db)\n",
    "        else:\n",
    "            edb = _lin_to_db(e)\n",
    "            over = edb - threshold_db\n",
    "            comp_db = over - (over / ratio)\n",
    "            g = _db_to_lin(-comp_db)\n",
    "        return g\n",
    "\n",
    "    if link_stereo:\n",
    "        gains = np.array([gain_curve(v) for v in env], dtype=np.float32)\n",
    "        y = np.column_stack([x[:,0]*gains, x[:,1]*gains]).astype(np.float32)\n",
    "    else:\n",
    "        gainsL = np.array([gain_curve(v) for v in envL], dtype=np.float32)\n",
    "        gainsR = np.array([gain_curve(v) for v in envR], dtype=np.float32)\n",
    "        y = np.column_stack([x[:,0]*gainsL, x[:,1]*gainsR]).astype(np.float32)\n",
    "\n",
    "    if makeup_db != 0.0:\n",
    "        y = apply_gain_db(y, makeup_db)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def transient_shaper(audio: np.ndarray, sr: int,\n",
    "                     attack_gain_db: float = 0.0, sustain_gain_db: float = 0.0,\n",
    "                     split_hz: float = 4000.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple transient shaper: high‑band emphasizes transient (attack), low‑band controls sustain.\n",
    "    Not a full envelope‑splitter, but useful as a primitive.\n",
    "    \"\"\"\n",
    "    x = _ensure_stereo(_sanitize(audio))\n",
    "    # Split into low/high\n",
    "    low = lowpass_filter(x, sr, cutoff_hz=split_hz, order=2)\n",
    "    high = x - low\n",
    "    # Envelope of high band ~ transients proxy\n",
    "    env_high = _envelope_detector(_mono(high), sr, attack_ms=2.0, release_ms=50.0)\n",
    "    att = apply_gain_db(high, attack_gain_db)\n",
    "    sus = apply_gain_db(low, sustain_gain_db)\n",
    "    y = att + sus\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "# --------- Fades ---------\n",
    "\n",
    "def fade_in(audio: np.ndarray, sr: int, dur_s: float = 0.01) -> np.ndarray:\n",
    "    x = _sanitize(audio)\n",
    "    n = int(max(1, dur_s * sr))\n",
    "    env = np.linspace(0.0, 1.0, n, dtype=np.float32)\n",
    "    y = x.copy()\n",
    "    if x.ndim == 1:\n",
    "        y[:n] *= env\n",
    "    else:\n",
    "        y[:n, :] *= env[:, None]\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def fade_out(audio: np.ndarray, sr: int, dur_s: float = 0.01) -> np.ndarray:\n",
    "    x = _sanitize(audio)\n",
    "    n = int(max(1, dur_s * sr))\n",
    "    env = np.linspace(1.0, 0.0, n, dtype=np.float32)\n",
    "    y = x.copy()\n",
    "    if x.ndim == 1:\n",
    "        y[-n:] *= env\n",
    "    else:\n",
    "        y[-n:, :] *= env[:, None]\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "print(\"DSP Primitives Layer loaded:\")\n",
    "print(\"- Gain/level: apply_gain_db, normalize_peak, normalize_lufs, measure_peak, measure_rms\")\n",
    "print(\"- Filters: highpass_filter, lowpass_filter, bandpass_filter, shelf_filter, peaking_eq, notch_filter, tilt_eq\")\n",
    "print(\"- Stereo: mid_side_encode, mid_side_decode, stereo_widener\")\n",
    "print(\"- Dynamics: compressor (soft‑knee), transient_shaper\")\n",
    "print(\"- Fades: fade_in, fade_out\")\n",
    "print(\"- K‑weighting/LUFS approx: k_weight, lufs_integrated_approx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b8140-c3bf-42dd-9f58-84754167eb63",
   "metadata": {},
   "source": [
    "### Processors (Feature Macros) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db319757-45e9-4cd4-834c-ad101d556a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched: render_from_cache now unpacks widen_stereo tuple correctly.\n",
      "Processors (Feature Macros) loaded: make_bassier, make_punchier, reduce_mud, add_air, widen_stereo, premaster_prep, build_preview_cache, render_from_cache.\n"
     ]
    }
   ],
   "source": [
    "# Processors (Feature Macros) — Notebook Layer\n",
    "# High-level, musical “dials” built on top of DSP primitives.\n",
    "# - Bass (low-shelf + optional dynamic control)\n",
    "# - Punch (kick/bass tightening via low-band ducking)\n",
    "# - Clarity (mud dip around 160–250 Hz)\n",
    "# - Air (HF shelf)\n",
    "# - Width (M/S scaling with safety)\n",
    "# - Pre‑master Prep (DC/sub cleanup + headroom target)\n",
    "# - Dial mapping 0–100 → safe internal params\n",
    "# - Fast preview via one-time preprocess cache\n",
    "#\n",
    "# Assumes the DSP primitives cell has been executed already.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Dial mapping helpers ----------\n",
    "\n",
    "def _map(amount: float, a0: float, a1: float) -> float:\n",
    "    \"\"\"Linear map amount (0..100) to [a0..a1].\"\"\"\n",
    "    amt = float(np.clip(amount, 0.0, 100.0))\n",
    "    return a0 + (a1 - a0) * (amt / 100.0)\n",
    "\n",
    "def _exp_map(amount: float, a0: float, a1: float) -> float:\n",
    "    \"\"\"Exponential-ish feel (more resolution at low values).\"\"\"\n",
    "    amt = float(np.clip(amount, 0.0, 100.0)) / 100.0\n",
    "    t = amt**1.6\n",
    "    return a0 + (a1 - a0) * t\n",
    "\n",
    "# ---------- Feature Macros (stateless) ----------\n",
    "\n",
    "def make_bassier(x: np.ndarray, sr: int, amount: float,\n",
    "                 base_hz: float = 80.0, max_db: float = 6.0,\n",
    "                 dynamic: bool = False) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Bass dial: low-shelf at ~80 Hz. amount=0..100 → 0..max_db dB.\n",
    "    If dynamic=True, add mild low-band compression to control boom.\n",
    "    \"\"\"\n",
    "    gain_db = _exp_map(amount, 0.0, max_db)  # gentle taper\n",
    "    y = shelf_filter(x, sr, cutoff_hz=base_hz, gain_db=gain_db, kind=\"low\", S=0.5)\n",
    "    params = {\"bass_gain_db\": round(gain_db, 2), \"bass_hz\": base_hz}\n",
    "    if dynamic and gain_db > 0.5:\n",
    "        # compress lows below ~120 Hz slightly (ratio 1.5–2.0)\n",
    "        low = lowpass_filter(y, sr, cutoff_hz=120.0, order=4)\n",
    "        low_c = compressor(low, sr, threshold_db=-28.0, ratio=1.8, attack_ms=10, release_ms=120, makeup_db=0.0)\n",
    "        y = y - low + low_c\n",
    "        params[\"bass_dynamic\"] = True\n",
    "    else:\n",
    "        params[\"bass_dynamic\"] = False\n",
    "    return y.astype(np.float32), params\n",
    "\n",
    "def make_punchier(x: np.ndarray, sr: int, amount: float,\n",
    "                  kick_lo: float = 40.0, kick_hi: float = 110.0,\n",
    "                  low_cutoff: float = 120.0) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Punch dial: duck non-kick lows under the kick envelope. amount 0..100 → 0..~6 dB depth.\n",
    "    \"\"\"\n",
    "    # Depth 0..6 dB, attack 3..6 ms, release 80..140 ms\n",
    "    depth_db = _map(amount, 0.0, 6.0)\n",
    "    atk_ms = _map(amount, 6.0, 3.0)\n",
    "    rel_ms = _map(amount, 80.0, 140.0)  # a bit longer release for higher amounts\n",
    "\n",
    "    # Build bands & envelope (single-pass per call; for fast preview use cache below)\n",
    "    low = lowpass_filter(x, sr, cutoff_hz=low_cutoff, order=4)\n",
    "    kick = bandpass_filter(x, sr, f_lo=kick_lo, f_hi=kick_hi, order=4)\n",
    "    nonkick_low = low - kick\n",
    "\n",
    "    env = _envelope_detector(_mono(kick), sr, attack_ms=4.0, release_ms=90.0)\n",
    "    # Normalize envelope to 0..1 robustly\n",
    "    p95 = np.percentile(env, 95) if env.size else 0.0\n",
    "    if p95 <= 1e-9:\n",
    "        gain_curve = np.ones_like(env, dtype=np.float32)\n",
    "    else:\n",
    "        env = np.clip(env / p95, 0.0, 1.0)\n",
    "        # Map to gain curve\n",
    "        floor_gain = 10**(-abs(depth_db)/20.0)\n",
    "        gain_curve = (floor_gain + (1.0 - floor_gain) * (1.0 - env)).astype(np.float32)\n",
    "    gain_curve = gain_curve[:, None]\n",
    "\n",
    "    ducked_nonkick = nonkick_low * gain_curve\n",
    "    lows_tight = ducked_nonkick + kick\n",
    "    high = x - low\n",
    "    y = lows_tight + high\n",
    "\n",
    "    params = {\"punch_depth_db\": round(depth_db, 2), \"kick_lo\": kick_lo, \"kick_hi\": kick_hi, \"low_cutoff\": low_cutoff,\n",
    "              \"attack_ms\": round(atk_ms, 1), \"release_ms\": round(rel_ms, 1)}\n",
    "    return y.astype(np.float32), params\n",
    "\n",
    "def reduce_mud(x: np.ndarray, sr: int, amount: float,\n",
    "               mud_hz_center: float = 200.0) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Clarity dial: dip around 160–250 Hz. amount 0..100 → 0..3 dB cut.\n",
    "    \"\"\"\n",
    "    cut_db = -_exp_map(amount, 0.0, 3.0)\n",
    "    hz = _map(amount, 180.0, 230.0)  # shift center slightly with amount\n",
    "    y = peaking_eq(x, sr, f0=hz, gain_db=cut_db, Q=1.0)\n",
    "    params = {\"mud_cut_db\": round(cut_db, 2), \"mud_hz\": round(hz, 1)}\n",
    "    return y.astype(np.float32), params\n",
    "\n",
    "def add_air(x: np.ndarray, sr: int, amount: float) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Air dial: high-shelf at ~10 kHz. amount 0..100 → 0..4 dB.\n",
    "    \"\"\"\n",
    "    db = _exp_map(amount, 0.0, 4.0)\n",
    "    y = shelf_filter(x, sr, cutoff_hz=10000.0, gain_db=db, kind=\"high\", S=0.5)\n",
    "    return y.astype(np.float32), {\"air_db\": round(db, 2), \"air_hz\": 10000.0}\n",
    "\n",
    "def widen_stereo(x: np.ndarray, sr: int, amount: float) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Width dial: scale side channel 1.0..1.4 while guarding mono-compat (soft limit).\n",
    "    \"\"\"\n",
    "    width = _map(amount, 1.0, 1.4)\n",
    "    # Soft limit if correlation is already low\n",
    "    M, S = mid_side_encode(x)\n",
    "    # Estimate correlation quickly\n",
    "    denom = np.maximum(1e-9, np.sqrt(M**2) * np.sqrt(S**2))\n",
    "    corr_est = float(np.mean((M * S) / denom))\n",
    "    if corr_est < 0.15:\n",
    "        width = min(width, 1.2)  # avoid over-wide if already decorrelated\n",
    "    y = mid_side_decode(M, S * width)\n",
    "    return y.astype(np.float32), {\"width_factor\": round(width, 3)}\n",
    "\n",
    "def premaster_prep(x: np.ndarray, sr: int,\n",
    "                   target_peak_dbfs: float = -6.0,\n",
    "                   hpf_hz: float = 20.0) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pre-master prep: gentle 20 Hz HPF + peak normalization to -6 dBFS.\n",
    "    \"\"\"\n",
    "    y = highpass_filter(x, sr, cutoff_hz=hpf_hz, order=2)\n",
    "    y = normalize_peak(y, target_dbfs=target_peak_dbfs)\n",
    "    return y.astype(np.float32), {\"hpf_hz\": hpf_hz, \"target_peak_dbfs\": target_peak_dbfs}\n",
    "\n",
    "# ---------- Fast preview via preprocess cache ----------\n",
    "\n",
    "@dataclass\n",
    "class PreviewCache:\n",
    "    sr: int\n",
    "    high: np.ndarray        # > low_cutoff\n",
    "    low: np.ndarray         # < low_cutoff\n",
    "    kick: np.ndarray        # kick band\n",
    "    env01: np.ndarray       # normalized kick envelope 0..1\n",
    "    low_cutoff: float\n",
    "    kick_lo: float\n",
    "    kick_hi: float\n",
    "\n",
    "def build_preview_cache(x: np.ndarray, sr: int,\n",
    "                        low_cutoff: float = 120.0,\n",
    "                        kick_lo: float = 40.0,\n",
    "                        kick_hi: float = 110.0) -> PreviewCache:\n",
    "    low = lowpass_filter(x, sr, cutoff_hz=low_cutoff, order=4)\n",
    "    high = x - low\n",
    "    kick = bandpass_filter(x, sr, f_lo=kick_lo, f_hi=kick_hi, order=4)\n",
    "    env = _envelope_detector(_mono(kick), sr, attack_ms=4.0, release_ms=90.0)\n",
    "    p95 = np.percentile(env, 95) if env.size else 0.0\n",
    "    env01 = np.zeros_like(env, dtype=np.float32) if p95 <= 1e-9 else np.clip(env / p95, 0.0, 1.0).astype(np.float32)\n",
    "    return PreviewCache(sr=sr, high=high.astype(np.float32), low=low.astype(np.float32),\n",
    "                        kick=kick.astype(np.float32), env01=env01,\n",
    "                        low_cutoff=low_cutoff, kick_lo=kick_lo, kick_hi=kick_hi)\n",
    "\n",
    "# Patch: fix render_from_cache width call (was returning a (array, params) tuple)\n",
    "# Now we correctly unpack the tuple so Y stays a numpy array.\n",
    "\n",
    "def render_from_cache(cache: PreviewCache,\n",
    "                      bass_amount: float = 0.0,\n",
    "                      punch_amount: float = 0.0,\n",
    "                      clarity_amount: float = 0.0,\n",
    "                      air_amount: float = 0.0,\n",
    "                      width_amount: float = 0.0,\n",
    "                      target_peak_dbfs: Optional[float] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Millisecond re-render from cached bands/envelope for interactive dials.\n",
    "    \"\"\"\n",
    "    # Start from separate bands\n",
    "    M = cache.low.copy()\n",
    "    H = cache.high.copy()\n",
    "    K = cache.kick.copy()\n",
    "    nonkick = M - K\n",
    "\n",
    "    # Bass: scale low band (acts like a shelf)\n",
    "    bass_db = _exp_map(bass_amount, 0.0, 6.0)\n",
    "    M = M * (10**(bass_db/20.0))\n",
    "\n",
    "    # Punch: duck non-kick lows with precomputed envelope\n",
    "    depth_db = _map(punch_amount, 0.0, 6.0)\n",
    "    floor_gain = 10**(-abs(depth_db)/20.0)\n",
    "    g_sc = (floor_gain + (1.0 - floor_gain) * (1.0 - cache.env01)).astype(np.float32)\n",
    "    ducked_nonkick = nonkick * g_sc[:, None]\n",
    "    lows_tight = ducked_nonkick + K\n",
    "\n",
    "    Y = lows_tight + H\n",
    "\n",
    "    # Clarity: light peaking dip ~200 Hz (approximate via single biquad now)\n",
    "    if clarity_amount > 0.0:\n",
    "        cut_db = -_exp_map(clarity_amount, 0.0, 3.0)\n",
    "        Y = peaking_eq(Y, cache.sr, f0=_map(clarity_amount, 180.0, 230.0), gain_db=cut_db, Q=1.0)\n",
    "\n",
    "    # Air: high shelf ~10 kHz\n",
    "    if air_amount > 0.0:\n",
    "        air_db = _exp_map(air_amount, 0.0, 4.0)\n",
    "        Y = shelf_filter(Y, cache.sr, cutoff_hz=10000.0, gain_db=air_db, kind=\"high\", S=0.5)\n",
    "\n",
    "    # Width: side scaling (correctly unpack tuple)\n",
    "    if width_amount > 0.0:\n",
    "        Y, _params_w = widen_stereo(Y, cache.sr, amount=width_amount)\n",
    "\n",
    "    params = {\n",
    "        \"bass_db\": round(bass_db, 2),\n",
    "        \"punch_depth_db\": round(depth_db, 2),\n",
    "        \"clarity_db\": round(-_exp_map(clarity_amount, 0.0, 3.0), 2) if clarity_amount>0 else 0.0,\n",
    "        \"air_db\": round(_exp_map(air_amount, 0.0, 4.0), 2) if air_amount>0 else 0.0,\n",
    "        \"width_amount\": round(width_amount, 2),\n",
    "    }\n",
    "\n",
    "    if target_peak_dbfs is not None:\n",
    "        Y = normalize_peak(Y, target_dbfs=target_peak_dbfs)\n",
    "\n",
    "    return Y.astype(np.float32), params\n",
    "\n",
    "print(\"Patched: render_from_cache now unpacks widen_stereo tuple correctly.\")\n",
    "\n",
    "\n",
    "print(\"Processors (Feature Macros) loaded: make_bassier, make_punchier, reduce_mud, add_air, widen_stereo, premaster_prep, build_preview_cache, render_from_cache.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155949d3-382d-4fc0-97d7-ae915de400be",
   "metadata": {},
   "source": [
    "### REDNER ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff31112-3064-4869-ade4-11d62751cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render Engine — Notebook Layer\n",
    "# Requires previous cells (I/O, Analysis, DSP Primitives, Processors) to be loaded.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# ---- Dial state (what the user controls) ----\n",
    "\n",
    "@dataclass\n",
    "class DialState:\n",
    "    bass: float = 0.0     # 0..100\n",
    "    punch: float = 0.0    # 0..100\n",
    "    clarity: float = 0.0  # 0..100\n",
    "    air: float = 0.0      # 0..100\n",
    "    width: float = 0.0    # 0..100\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    low_cutoff: float = 120.0\n",
    "    kick_lo: float = 40.0\n",
    "    kick_hi: float = 110.0\n",
    "\n",
    "@dataclass\n",
    "class RenderOptions:\n",
    "    target_peak_dbfs: Optional[float] = -1.0   # normalize peak at the end; set None to skip\n",
    "    hpf_hz: Optional[float] = None             # optional extra HPF before normalize (None = skip)\n",
    "    bit_depth: str = \"PCM_24\"                  # \"PCM_24\" (recommended), \"FLOAT\", \"PCM_16\"\n",
    "    save_headroom_first: bool = False          # if True, do a premaster-style -6 dB pass before dials\n",
    "\n",
    "# ---- Render Engine ----\n",
    "\n",
    "class RenderEngine:\n",
    "    def __init__(self, x: np.ndarray, sr: int, preprocess: Optional[PreprocessConfig] = None):\n",
    "        \"\"\"\n",
    "        x: input stereo/mono numpy array\n",
    "        sr: sample rate\n",
    "        preprocess: parameters for cache building (bands + kick envelope)\n",
    "        \"\"\"\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.sr = int(sr)\n",
    "        self.pre_cfg = preprocess or PreprocessConfig()\n",
    "        self.cache = None  # filled by self.preprocess()\n",
    "    \n",
    "    def preprocess(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build fast preview cache (low/high split, kick band, envelope).\"\"\"\n",
    "        self.cache = build_preview_cache(\n",
    "            self.x, self.sr,\n",
    "            low_cutoff=self.pre_cfg.low_cutoff,\n",
    "            kick_lo=self.pre_cfg.kick_lo,\n",
    "            kick_hi=self.pre_cfg.kick_hi\n",
    "        )\n",
    "        return {\n",
    "            \"sr\": self.sr,\n",
    "            \"n_samples\": int(self.x.shape[0]),\n",
    "            \"low_cutoff\": self.pre_cfg.low_cutoff,\n",
    "            \"kick_lo\": self.pre_cfg.kick_lo,\n",
    "            \"kick_hi\": self.pre_cfg.kick_hi\n",
    "        }\n",
    "    \n",
    "    def _ensure_cache(self):\n",
    "        if self.cache is None:\n",
    "            self.preprocess()\n",
    "\n",
    "    # ---------- PREVIEW ----------\n",
    "    def preview(self,\n",
    "                dials: DialState,\n",
    "                start_s: float = 0.0,\n",
    "                dur_s: Optional[float] = 30.0,\n",
    "                opts: Optional[RenderOptions] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fast preview using cached bands/envelope. Optional time window.\n",
    "        Returns (audio_preview, params).\n",
    "        \"\"\"\n",
    "        self._ensure_cache()\n",
    "        opts = opts or RenderOptions()\n",
    "        \n",
    "        # window the cached arrays (no re-filtering)\n",
    "        n0 = int(max(0, start_s * self.sr))\n",
    "        n1 = int(self.cache.high.shape[0]) if dur_s is None else int(min(self.cache.high.shape[0], n0 + dur_s * self.sr))\n",
    "        \n",
    "        # build a temporary mini-cache slice for fast render\n",
    "        slice_cache = type(self.cache)(\n",
    "            sr=self.cache.sr,\n",
    "            high=self.cache.high[n0:n1],\n",
    "            low=self.cache.low[n0:n1],\n",
    "            kick=self.cache.kick[n0:n1],\n",
    "            env01=self.cache.env01[n0:n1],\n",
    "            low_cutoff=self.cache.low_cutoff,\n",
    "            kick_lo=self.cache.kick_lo,\n",
    "            kick_hi=self.cache.kick_hi\n",
    "        )\n",
    "        \n",
    "        y, params = render_from_cache(\n",
    "            slice_cache,\n",
    "            bass_amount=dials.bass,\n",
    "            punch_amount=dials.punch,\n",
    "            clarity_amount=dials.clarity,\n",
    "            air_amount=dials.air,\n",
    "            width_amount=dials.width,\n",
    "            target_peak_dbfs=opts.target_peak_dbfs\n",
    "        )\n",
    "        return y, params\n",
    "\n",
    "    # ---------- COMMIT (FULL RENDER) ----------\n",
    "    def commit(self,\n",
    "               out_path: str,\n",
    "               dials: DialState,\n",
    "               opts: Optional[RenderOptions] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Full-length render using the precomputed cache (fast) and export to disk.\n",
    "        Returns a dict of render metadata.\n",
    "        \"\"\"\n",
    "        self._ensure_cache()\n",
    "        opts = opts or RenderOptions()\n",
    "        \n",
    "        # optional premaster headroom first (use the primitive so it HPFs and normalizes)\n",
    "        x_work = self.x\n",
    "        pre_meta = None\n",
    "        if opts.save_headroom_first:\n",
    "            x_work, pre_meta = premaster_prep(x_work, self.sr, target_peak_dbfs=-6.0, hpf_hz=20.0)\n",
    "\n",
    "            # If we premastered first, rebuild cache so dials work on the premastered signal\n",
    "            self.x = x_work\n",
    "            self.preprocess()\n",
    "\n",
    "        # dial render over the *full* cache\n",
    "        y, params = render_from_cache(\n",
    "            self.cache,\n",
    "            bass_amount=dials.bass,\n",
    "            punch_amount=dials.punch,\n",
    "            clarity_amount=dials.clarity,\n",
    "            air_amount=dials.air,\n",
    "            width_amount=dials.width,\n",
    "            target_peak_dbfs=opts.target_peak_dbfs\n",
    "        )\n",
    "        \n",
    "        # optional extra HPF after dials\n",
    "        if opts.hpf_hz is not None:\n",
    "            y = highpass_filter(y, self.sr, cutoff_hz=opts.hpf_hz, order=2)\n",
    "\n",
    "        # export\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(out_path)), exist_ok=True)\n",
    "        subtype = {\n",
    "            \"PCM_24\": \"PCM_24\",\n",
    "            \"PCM_16\": \"PCM_16\",\n",
    "            \"FLOAT\": \"FLOAT\"\n",
    "        }.get(opts.bit_depth.upper(), \"PCM_24\")\n",
    "        sf.write(out_path, y, self.sr, subtype=subtype)\n",
    "\n",
    "        meta = {\n",
    "            \"sr\": self.sr,\n",
    "            \"samples\": int(y.shape[0]),\n",
    "            \"dials\": asdict(dials),\n",
    "            \"preprocess\": asdict(self.pre_cfg),\n",
    "            \"params\": params,\n",
    "            \"options\": asdict(opts),\n",
    "            \"out_path\": os.path.abspath(out_path),\n",
    "            \"bit_depth\": subtype\n",
    "        }\n",
    "        if pre_meta:\n",
    "            meta[\"premaster_first\"] = pre_meta\n",
    "        return meta\n",
    "\n",
    "    # ---------- BATCH VARIANTS ----------\n",
    "    def commit_variants(self,\n",
    "                        base_outdir: str,\n",
    "                        variants: List[Tuple[str, DialState]],\n",
    "                        opts: Optional[RenderOptions] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Render multiple named variants and return their metadata.\n",
    "        variants: list of (name, DialState)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for name, d in variants:\n",
    "            out_path = os.path.join(base_outdir, f\"{name}.wav\")\n",
    "            info = self.commit(out_path, dials=d, opts=opts)\n",
    "            results.append(info)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eacef16-3144-40ba-99c6-102329f4c199",
   "metadata": {},
   "source": [
    "### Pre-Master Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62817cf-051b-4463-be1f-3f28e23fc54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pre-Master Prep Layer ---\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def sanitize_audio(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Replace NaN/Inf and clamp extreme outliers.\"\"\"\n",
    "    return np.clip(np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0),\n",
    "                   -4.0, 4.0).astype(np.float32)\n",
    "\n",
    "def highpass_filter(audio: np.ndarray, sr: int, cutoff_hz: float = 20.0, order: int = 2) -> np.ndarray:\n",
    "    \"\"\"Gentle high-pass to clear subsonics.\"\"\"\n",
    "    from scipy.signal import butter, filtfilt\n",
    "    b, a = butter(order, cutoff_hz / (sr/2.0), btype='highpass')\n",
    "    return filtfilt(b, a, audio, axis=0)\n",
    "\n",
    "def normalize_peak(audio: np.ndarray, target_dbfs: float = -6.0, eps: float = 1e-9) -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Scale so peak hits target_dbfs. Returns (scaled_audio, applied_gain_db).\"\"\"\n",
    "    x = sanitize_audio(audio)\n",
    "    peak = float(np.max(np.abs(x)))\n",
    "    if peak < eps:\n",
    "        return x, 0.0\n",
    "    current_db = 20 * np.log10(peak + eps)\n",
    "    gain_db = target_dbfs - current_db\n",
    "    gain_lin = 10 ** (gain_db/20)\n",
    "    return (x * gain_lin).astype(np.float32), gain_db\n",
    "\n",
    "def premaster_prep(audio: np.ndarray, sr: int,\n",
    "                   target_peak_dbfs: float = -6.0,\n",
    "                   hpf_hz: float = 20.0) -> tuple[np.ndarray, dict]:\n",
    "    \"\"\"Do full pre-master prep: sanitize, HPF, normalize to headroom.\"\"\"\n",
    "    y = sanitize_audio(audio)\n",
    "    if hpf_hz:\n",
    "        y = highpass_filter(y, sr, cutoff_hz=hpf_hz)\n",
    "    y, gain_db = normalize_peak(y, target_dbfs=target_peak_dbfs)\n",
    "\n",
    "    meta = {\n",
    "        \"target_peak_dbfs\": target_peak_dbfs,\n",
    "        \"applied_gain_db\": round(gain_db, 2),\n",
    "        \"hpf_hz\": hpf_hz,\n",
    "        \"sr\": sr,\n",
    "        \"peak_after\": round(float(np.max(np.abs(y))), 4)\n",
    "    }\n",
    "    return y, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e53b9-13f2-41fb-9eb9-18a72e464e9e",
   "metadata": {},
   "source": [
    "### Mastering Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5af703ee-3f52-43c3-a0cf-ed45b789e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mastering Orchestrator — Notebook Layer\n",
    "# Provider-agnostic runner that:\n",
    "#  - accepts a pre-master WAV\n",
    "#  - runs 1..N mastering providers (local + external)\n",
    "#  - collects outputs, level-matches (optional), and registers artifacts\n",
    "#\n",
    "# Includes:\n",
    "#  - LocalMasterProvider: simple \"house master\" with a few styles (neutral/warm/bright/loud)\n",
    "#  - LandrProvider (stub): method contracts to implement when you wire real API calls\n",
    "\n",
    "# ---- SAFE Mastering patch: limiter + toned-down styles ----\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Any\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "def _sanitize(x): \n",
    "    return np.clip(np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0), -4.0, 4.0).astype(np.float32)\n",
    "\n",
    "def _db_to_lin(db): return 10.0**(db/20.0)\n",
    "\n",
    "def true_peak_db_approx(x: np.ndarray, sr: int, oversample: int = 4) -> float:\n",
    "    x_os = signal.resample_poly(_sanitize(x), oversample, 1, axis=0 if x.ndim>1 else 0)\n",
    "    tp = float(np.max(np.abs(x_os)))\n",
    "    return 20*np.log10(max(1e-12, tp))\n",
    "\n",
    "def normalize_true_peak(x: np.ndarray, sr: int, target_dbtp: float = -1.0) -> np.ndarray:\n",
    "    tp = true_peak_db_approx(x, sr, oversample=4)\n",
    "    gain_db = target_dbtp - tp\n",
    "    return (_sanitize(x) * _db_to_lin(gain_db)).astype(np.float32)\n",
    "\n",
    "# Gentle shelves/peaks (unchanged; just keep amounts conservative)\n",
    "def lowshelf(x, sr, hz, db): \n",
    "    A = 10**(db/40.0)\n",
    "    w0 = 2*np.pi*hz/sr; cosw0=np.cos(w0); sinw0=np.sin(w0); S=0.5\n",
    "    alpha = sinw0/2*np.sqrt((A+1/A)*(1/S-1)+2)\n",
    "    b0=A*((A+1)-(A-1)*cosw0+2*np.sqrt(A)*alpha)\n",
    "    b1=2*A*((A-1)-(A+1)*cosw0)\n",
    "    b2=A*((A+1)-(A-1)*cosw0-2*np.sqrt(A)*alpha)\n",
    "    a0=(A+1)+(A-1)*cosw0+2*np.sqrt(A)*alpha\n",
    "    a1=-2*((A-1)+(A+1)*cosw0)\n",
    "    a2=(A+1)+(A-1)*cosw0-2*np.sqrt(A)*alpha\n",
    "    sos = signal.tf2sos([b0/a0,b1/a0,b2/a0],[1.0,a1/a0,a2/a0])\n",
    "    return signal.sosfilt(sos,_sanitize(x),axis=0 if x.ndim>1 else 0).astype(np.float32)\n",
    "\n",
    "def highshelf(x, sr, hz, db):\n",
    "    A = 10**(db/40.0)\n",
    "    w0 = 2*np.pi*hz/sr; cosw0=np.cos(w0); sinw0=np.sin(w0); S=0.5\n",
    "    alpha = sinw0/2*np.sqrt((A+1/A)*(1/S-1)+2)\n",
    "    b0=A*((A+1)+(A-1)*cosw0+2*np.sqrt(A)*alpha)\n",
    "    b1=-2*A*((A-1)+(A+1)*cosw0)\n",
    "    b2=A*((A+1)+(A-1)*cosw0)\n",
    "    a0=(A+1)-(A-1)*cosw0+2*np.sqrt(A)*alpha\n",
    "    a1=2*((A-1)-(A+1)*cosw0)\n",
    "    a2=(A+1)-(A-1)*cosw0-2*np.sqrt(A)*alpha\n",
    "    sos = signal.tf2sos([b0/a0,b1/a0,b2/a0],[1.0,a1/a0,a2/a0])\n",
    "    return signal.sosfilt(sos,_sanitize(x),axis=0 if x.ndim>1 else 0).astype(np.float32)\n",
    "\n",
    "def broad_peak(x, sr, f0, db, Q=0.7):\n",
    "    A = 10**(db/40.0)\n",
    "    w0=2*np.pi*f0/sr; alpha=np.sin(w0)/(2*Q); cosw0=np.cos(w0)\n",
    "    b0=1+alpha*A; b1=-2*cosw0; b2=1-alpha*A\n",
    "    a0=1+alpha/A; a1=-2*cosw0; a2=1-alpha/A\n",
    "    sos = signal.tf2sos([b0/a0,b1/a0,b2/a0],[1.0,a1/a0,a2/a0])\n",
    "    return signal.sosfilt(sos,_sanitize(x),axis=0 if x.ndim>1 else 0).astype(np.float32)\n",
    "\n",
    "# --- new: lookahead soft-knee limiter (no screechy clamp) ---\n",
    "def lookahead_limiter(x: np.ndarray, sr: int,\n",
    "                      ceiling_dbfs: float = -1.0,\n",
    "                      lookahead_ms: float = 2.0,\n",
    "                      attack_ms: float = 1.0,\n",
    "                      release_ms: float = 50.0,\n",
    "                      knee_db: float = 1.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Feed-forward, soft-knee, lookahead limiter. Mono/stereo safe.\n",
    "    \"\"\"\n",
    "    x = _sanitize(x)\n",
    "    la = max(1, int(sr * lookahead_ms/1000.0))\n",
    "    c = _db_to_lin(ceiling_dbfs)\n",
    "    knee = _db_to_lin(-abs(knee_db))  # knee expressed as a soft blend near the ceiling\n",
    "\n",
    "    # Lookahead via simple delay\n",
    "    if x.ndim == 1:\n",
    "        pad = np.zeros(la, dtype=x.dtype); x_del = np.concatenate([pad, x])\n",
    "        x_for_det = np.concatenate([x, pad])\n",
    "    else:\n",
    "        pad = np.zeros((la, x.shape[1]), dtype=x.dtype); x_del = np.vstack([pad, x])\n",
    "        x_for_det = np.vstack([x, pad])\n",
    "\n",
    "    # Peak detector with attack/release\n",
    "    atk = np.exp(-1.0 / max(1, int(sr*attack_ms/1000.0)))\n",
    "    rel = np.exp(-1.0 / max(1, int(sr*release_ms/1000.0)))\n",
    "    env = np.zeros_like(x_for_det, dtype=np.float32)\n",
    "    mag = np.abs(x_for_det)\n",
    "    if x.ndim == 1:\n",
    "        e = 0.0\n",
    "        for n in range(len(mag)):\n",
    "            e = max(mag[n], e* (atk if mag[n] > e else rel))\n",
    "            env[n] = e\n",
    "    else:\n",
    "        e = np.zeros(x.shape[1], dtype=np.float32)\n",
    "        for n in range(len(mag)):\n",
    "            cur = mag[n]\n",
    "            e = np.maximum(cur, e*(atk if np.any(cur>e) else rel))\n",
    "            env[n] = e\n",
    "\n",
    "    # Gain computer\n",
    "    # soft knee near the ceiling: reduce ratio smoothly as we approach c\n",
    "    eps = 1e-12\n",
    "    over = np.maximum(0.0, env - c)\n",
    "    knee_mix = (env / (env + knee*c + eps))\n",
    "    raw_gain = c / (env + eps)\n",
    "    gain = 1.0 - knee_mix + knee_mix * np.minimum(1.0, raw_gain)\n",
    "\n",
    "    # Apply gain to delayed signal, trim back to original length\n",
    "    y = (x_del * gain[:len(x_del)]).astype(np.float32)\n",
    "    y = y[la: la + len(x)]\n",
    "    return y\n",
    "\n",
    "# --- patched LocalMasterProvider using safe limiter and milder EQ ---\n",
    "class LocalMasterProvider(MasteringProvider):\n",
    "    name = \"local\"\n",
    "    def __init__(self, bit_depth: str = \"PCM_24\"):\n",
    "        self.bit_depth = bit_depth\n",
    "\n",
    "    def _process(self, x: np.ndarray, sr: int, style: str, strength: float) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        s = float(np.clip(strength, 0.0, 1.0))\n",
    "        y = _sanitize(x)\n",
    "\n",
    "        # tiny safety headroom before EQ\n",
    "        y = normalize_true_peak(y, sr, target_dbtp=-2.5)\n",
    "\n",
    "        # toned-down styles (keep boosts small; avoid big >10 kHz lifts)\n",
    "        if style == \"neutral\":\n",
    "            y = highshelf(y, sr, 9000, +0.8*s)\n",
    "            y = lowshelf(y, sr, 90, +0.5*s)\n",
    "            glue = 0.10 + 0.10*s\n",
    "        elif style == \"warm\":\n",
    "            y = lowshelf(y, sr, 120, +1.2*s)\n",
    "            y = broad_peak(y, sr, 3500, -0.6*s, Q=1.0)\n",
    "            glue = 0.12 + 0.12*s\n",
    "        elif style == \"bright\":\n",
    "            # cap bright lift and keep the shelf lower (8–10 kHz) to avoid fizz\n",
    "            y = highshelf(y, sr, 8500, +1.4*s)\n",
    "            y = broad_peak(y, sr, 220, -0.5*s, Q=0.9)\n",
    "            glue = 0.10 + 0.12*s\n",
    "        elif style == \"loud\":\n",
    "            y = highshelf(y, sr, 9000, +1.0*s)\n",
    "            y = lowshelf(y, sr, 90, +0.8*s)\n",
    "            glue = 0.16 + 0.18*s\n",
    "        else:\n",
    "            style = \"neutral\"\n",
    "            glue = 0.10 + 0.10*s\n",
    "\n",
    "        # \"glue\" via parallel into limiter (safe, lookahead)\n",
    "        limited = lookahead_limiter(y, sr, ceiling_dbfs=-1.2, lookahead_ms=2.0, attack_ms=1.0, release_ms=60.0, knee_db=1.5)\n",
    "        y = (1.0 - glue)*y + glue*limited\n",
    "\n",
    "        # Final true-peak trim to -1.0 dBTP (prevents inter-sample spikes)\n",
    "        y = normalize_true_peak(y, sr, target_dbtp=-1.0)\n",
    "\n",
    "        params = {\n",
    "            \"style\": style,\n",
    "            \"strength\": s,\n",
    "            \"glue\": round(glue, 3),\n",
    "            \"true_peak_dbtp\": round(true_peak_db_approx(y, sr), 3)\n",
    "        }\n",
    "        return y.astype(np.float32), params\n",
    "\n",
    "    def submit(self, req: MasterRequest) -> str:\n",
    "        return \"local-sync\"\n",
    "\n",
    "    def run_sync(self, req: MasterRequest, out_path: str) -> MasterResult:\n",
    "        y, sr = sf.read(req.input_path)\n",
    "        y_proc, params = self._process(y, sr, req.style, req.strength)\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(out_path)), exist_ok=True)\n",
    "        sf.write(out_path, y_proc, sr, subtype=self.bit_depth)\n",
    "        return MasterResult(provider=self.name, style=req.style, strength=req.strength,\n",
    "                            out_path=os.path.abspath(out_path), sr=sr, bit_depth=self.bit_depth, params=params)\n",
    "\n",
    "# --- LANDR provider (stub) ---\n",
    "class LandrProvider(MasteringProvider):\n",
    "    \"\"\"\n",
    "    Stubbed adapter. Fill in with real API calls when ready:\n",
    "      - __init__(api_key: str)\n",
    "      - submit(): upload pre-master, select style/strength, returns job_id\n",
    "      - poll(job_id): query job status (\"queued\"|\"processing\"|\"done\"|\"error\")\n",
    "      - download(job_id, out_path): fetch mastered WAV to out_path\n",
    "    \"\"\"\n",
    "    name = \"landr\"\n",
    "    def __init__(self, api_key: Optional[str] = None, bit_depth: str = \"PCM_24\"):\n",
    "        self.api_key = api_key or os.environ.get(\"LANDR_API_KEY\", None)\n",
    "        self.bit_depth = bit_depth\n",
    "        # self.endpoint = \"https://api.landr.com/...\"  # example placeholder\n",
    "\n",
    "    def submit(self, req: MasterRequest) -> str:\n",
    "        raise NotImplementedError(\"LANDR adapter not wired yet. Implement API call here.\")\n",
    "\n",
    "    def poll(self, job_id: str) -> str:\n",
    "        raise NotImplementedError(\"LANDR adapter not wired yet. Implement job status polling.\")\n",
    "\n",
    "    def download(self, job_id: str, out_path: str) -> MasterResult:\n",
    "        raise NotImplementedError(\"LANDR adapter not wired yet. Implement download to out_path.\")\n",
    "\n",
    "# --- Orchestrator ---\n",
    "class MasteringOrchestrator:\n",
    "    \"\"\"\n",
    "    Runs 1..N providers for a given pre-master and registers outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, workspace_paths, manifest):\n",
    "        self.paths = workspace_paths\n",
    "        self.man = manifest\n",
    "\n",
    "    def run(self,\n",
    "            premaster_path: str,\n",
    "            providers: List[MasteringProvider],\n",
    "            styles: List[Tuple[str,float]],     # list of (style, strength 0..1)\n",
    "            out_tag: str = \"master\",\n",
    "            level_match_preview_lufs: Optional[float] = None  # if set, write *preview* copies level-matched for A/B\n",
    "            ) -> List[MasterResult]:\n",
    "\n",
    "        results: List[MasterResult] = []\n",
    "        base_outdir = os.path.join(self.paths.outputs, out_tag)\n",
    "        os.makedirs(base_outdir, exist_ok=True)\n",
    "\n",
    "        for prov in providers:\n",
    "            for style, strength in styles:\n",
    "                name = f\"{prov.name}_{style}_{int(round(strength*100))}\"\n",
    "                out_path = os.path.join(base_outdir, f\"{name}.wav\")\n",
    "\n",
    "                if isinstance(prov, LocalMasterProvider):\n",
    "                    res = prov.run_sync(MasterRequest(premaster_path, style=style, strength=strength), out_path)\n",
    "                else:\n",
    "                    # external provider flow (submit -> poll -> download)\n",
    "                    job_id = prov.submit(MasterRequest(premaster_path, style=style, strength=strength))\n",
    "                    status = prov.poll(job_id)\n",
    "                    while status not in (\"done\", \"error\"):\n",
    "                        time.sleep(2.0)\n",
    "                        status = prov.poll(job_id)\n",
    "                    if status == \"error\":\n",
    "                        print(f\"[{prov.name}] job failed for style={style} strength={strength}\")\n",
    "                        continue\n",
    "                    res = prov.download(job_id, out_path)\n",
    "\n",
    "                # register artifact\n",
    "                register_artifact(self.man, res.out_path, kind=out_tag, params={\n",
    "                    \"provider\": res.provider,\n",
    "                    \"style\": res.style,\n",
    "                    \"strength\": res.strength,\n",
    "                    **res.params\n",
    "                }, stage=name)\n",
    "\n",
    "                results.append(res)\n",
    "\n",
    "                # optional preview copies level-matched to a LUFS target (for A/B only)\n",
    "                if level_match_preview_lufs is not None:\n",
    "                    # lightweight LUFS approx + gain\n",
    "                    from scipy import signal\n",
    "                    def k_weight(mono, sr):\n",
    "                        # simple K-weight from earlier; inline here for convenience\n",
    "                        sos_hp = signal.butter(2, 38.0/(sr*0.5), btype='highpass', output='sos')\n",
    "                        y = signal.sosfilt(sos_hp, mono)\n",
    "                        return y\n",
    "                    x, sr = sf.read(res.out_path)\n",
    "                    mono = x if x.ndim==1 else np.mean(x, axis=1)\n",
    "                    xk = k_weight(mono, sr)\n",
    "                    ms = float(np.mean(xk**2)); cur_lufs = -0.691 + 10*np.log10(max(1e-12, ms))\n",
    "                    delta = level_match_preview_lufs - cur_lufs\n",
    "                    x_matched = (x * _db_to_lin(delta)).astype(np.float32)\n",
    "                    # keep true peak safe\n",
    "                    x_matched = normalize_true_peak(x_matched, sr, target_dbtp=-1.0)\n",
    "                    prev_path = os.path.join(base_outdir, f\"{name}__LM{int(level_match_preview_lufs)}LUFS.wav\")\n",
    "                    sf.write(prev_path, x_matched, sr, subtype=res.bit_depth)\n",
    "                    register_artifact(self.man, prev_path, kind=f\"{out_tag}_preview\", params={\n",
    "                        \"provider\": res.provider, \"style\": res.style, \"strength\": res.strength,\n",
    "                        \"level_matched_lufs\": level_match_preview_lufs\n",
    "                    }, stage=f\"{name}__preview\")\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bedd37e-319d-49f1-84d3-b4e0a927425b",
   "metadata": {},
   "source": [
    "### Streaming Normalization Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b236ac9f-c17e-45a5-92c3-f045bd42f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Normalization Simulator — Notebook Layer\n",
    "# Requires: I/O + Analysis layer (for lufs_integrated_approx), and manifest helpers.\n",
    "# Outputs \"as-heard\" WAVs for each platform and returns a summary table.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "\n",
    "# ---- Reuse analysis helpers if present; else provide fallbacks ----\n",
    "def _sanitize(x: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0), -4.0, 4.0).astype(np.float32)\n",
    "\n",
    "def _db_to_lin(db: float) -> float:\n",
    "    return 10.0**(db/20.0)\n",
    "\n",
    "def _lin_to_db(l: float, eps: float = 1e-12) -> float:\n",
    "    return 20.0*np.log10(max(eps, abs(l)))\n",
    "\n",
    "def _k_weighting_sos(sr: int):\n",
    "    f_hp = 38.0\n",
    "    sos_hp = signal.butter(2, f_hp/(sr*0.5), btype=\"highpass\", output=\"sos\")\n",
    "    # ~+4 dB shelf around 1 kHz\n",
    "    f_shelf = 1681.974450955533; Q_shelf = 0.7071752369554196; gain_db = 3.99984385397\n",
    "    A = 10**(gain_db/40.0); w0 = 2*np.pi*f_shelf/sr\n",
    "    alpha = np.sin(w0)/(2*Q_shelf); cosw0 = np.cos(w0)\n",
    "    b0 =    A*((A+1) + (A-1)*cosw0 + 2*np.sqrt(A)*alpha)\n",
    "    b1 = -2*A*((A-1) + (A+1)*cosw0)\n",
    "    b2 =    A*((A+1) + (A-1)*cosw0 - 2*np.sqrt(A)*alpha)\n",
    "    a0 =        (A+1) - (A-1)*cosw0 + 2*np.sqrt(A)*alpha\n",
    "    a1 =    2*((A-1) - (A+1)*cosw0)\n",
    "    a2 =        (A+1) - (A-1)*cosw0 - 2*np.sqrt(A)*alpha\n",
    "    sos_sh = signal.tf2sos([b0/a0, b1/a0, b2/a0], [1.0, a1/a0, a2/a0])\n",
    "    return np.vstack([sos_hp, sos_sh])\n",
    "\n",
    "def _lufs_integrated_approx(x: np.ndarray, sr: int) -> float:\n",
    "    # Use existing function if present\n",
    "    try:\n",
    "        return lufs_integrated_approx(x, sr)  # defined in your Analysis layer\n",
    "    except NameError:\n",
    "        mono = x if x.ndim == 1 else np.mean(x, axis=1)\n",
    "        sos = _k_weighting_sos(sr)\n",
    "        xk = signal.sosfilt(sos, mono)\n",
    "        ms = float(np.mean(xk**2))\n",
    "        return -0.691 + 10.0*np.log10(max(1e-12, ms))\n",
    "\n",
    "def _true_peak_dbtp(x: np.ndarray, sr: int, oversample: int = 4) -> float:\n",
    "    try:\n",
    "        # Prefer previously defined function name if present\n",
    "        return true_peak_dbfs(x, sr, oversample=oversample)  # returns dBFS (≈ dBTP here)\n",
    "    except NameError:\n",
    "        x_os = signal.resample_poly(_sanitize(x), oversample, 1, axis=0 if x.ndim>1 else 0)\n",
    "        tp = float(np.max(np.abs(x_os)))\n",
    "        return 20.0*np.log10(max(1e-12, tp))\n",
    "\n",
    "def _normalize_true_peak(x: np.ndarray, sr: int, target_dbtp: float = -1.0, oversample: int = 4) -> Tuple[np.ndarray, float]:\n",
    "    tp = _true_peak_dbtp(x, sr, oversample=oversample)\n",
    "    gain_db = target_dbtp - tp\n",
    "    y = (_sanitize(x) * _db_to_lin(gain_db)).astype(np.float32)\n",
    "    return y, gain_db\n",
    "\n",
    "def _gentle_limiter(x: np.ndarray, ceiling_dbfs: float = -1.0, knee_db: float = 0.8) -> np.ndarray:\n",
    "    # super-simple soft ceiling; not a brickwall TP limiter (good enough for preview)\n",
    "    c = _db_to_lin(ceiling_dbfs)\n",
    "    y = _sanitize(x).copy()\n",
    "    mag = np.abs(y)\n",
    "    over = np.maximum(0.0, mag - c)\n",
    "    knee = _db_to_lin(-knee_db)\n",
    "    over = over / (1.0 + (over / (knee*c))**2)\n",
    "    y = np.sign(y) * np.minimum(mag, c)  # clamp\n",
    "    # gentle blend to reduce clicks\n",
    "    return (0.6*_sanitize(x) + 0.4*y).astype(np.float32)\n",
    "\n",
    "# ---- Profiles ----\n",
    "@dataclass\n",
    "class StreamingProfile:\n",
    "    name: str\n",
    "    target_lufs: float       # platform loudness target (track mode)\n",
    "    tp_ceiling_db: float     # approximate true-peak ceiling\n",
    "    tp_strategy: str = \"trim\"  # \"trim\" (reduce gain) or \"limit\" (lightly limit)\n",
    "\n",
    "def default_streaming_profiles() -> Dict[str, StreamingProfile]:\n",
    "    # Typical/commonly-cited targets (approximate, for preview). Override as needed.\n",
    "    return {\n",
    "        \"Spotify\":     StreamingProfile(\"Spotify\",     target_lufs=-14.0, tp_ceiling_db=-1.0, tp_strategy=\"trim\"),\n",
    "        \"AppleMusic\":  StreamingProfile(\"AppleMusic\",  target_lufs=-16.0, tp_ceiling_db=-1.0, tp_strategy=\"trim\"),\n",
    "        \"YouTube\":     StreamingProfile(\"YouTube\",     target_lufs=-14.0, tp_ceiling_db=-1.0, tp_strategy=\"trim\"),\n",
    "        \"TIDAL\":       StreamingProfile(\"TIDAL\",       target_lufs=-14.0, tp_ceiling_db=-1.0, tp_strategy=\"trim\"),\n",
    "        \"Amazon\":      StreamingProfile(\"Amazon\",      target_lufs=-14.0, tp_ceiling_db=-1.0, tp_strategy=\"trim\"),\n",
    "    }\n",
    "\n",
    "# ---- Core simulation ----\n",
    "def simulate_streaming_as_heard(x: np.ndarray, sr: int, profile: StreamingProfile,\n",
    "                                oversample_tp: int = 4) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return an 'as-heard' version after platform gain normalization and TP guard.\n",
    "    \"\"\"\n",
    "    x = _sanitize(x)\n",
    "    in_lufs = _lufs_integrated_approx(x, sr)\n",
    "    in_tp = _true_peak_dbtp(x, sr, oversample=oversample_tp)\n",
    "\n",
    "    # 1) loudness normalization (pure gain)\n",
    "    delta_db = profile.target_lufs - in_lufs\n",
    "    y = (x * _db_to_lin(delta_db)).astype(np.float32)\n",
    "\n",
    "    # 2) true-peak guard\n",
    "    after_tp = _true_peak_dbtp(y, sr, oversample=oversample_tp)\n",
    "    tp_over = after_tp - profile.tp_ceiling_db\n",
    "    tp_action = None\n",
    "    trim_db = 0.0\n",
    "\n",
    "    if tp_over > 0.0:\n",
    "        if profile.tp_strategy == \"limit\":\n",
    "            y = _gentle_limiter(y, ceiling_dbfs=profile.tp_ceiling_db, knee_db=0.8)\n",
    "            tp_action = \"limit\"\n",
    "        else:\n",
    "            # trim enough to meet the ceiling\n",
    "            y, trim_db = _normalize_true_peak(y, sr, target_dbtp=profile.tp_ceiling_db, oversample=oversample_tp)\n",
    "            tp_action = \"trim\"\n",
    "\n",
    "    out_lufs = _lufs_integrated_approx(y, sr)\n",
    "    out_tp = _true_peak_dbtp(y, sr, oversample=oversample_tp)\n",
    "\n",
    "    meta = {\n",
    "        \"profile\": asdict(profile),\n",
    "        \"in_lufs\": round(in_lufs, 2),\n",
    "        \"in_true_peak_dbTP\": round(in_tp, 2),\n",
    "        \"gain_to_target_db\": round(delta_db, 2),\n",
    "        \"tp_action\": tp_action,\n",
    "        \"extra_trim_db\": round(trim_db, 2) if tp_action == \"trim\" else 0.0,\n",
    "        \"out_lufs\": round(out_lufs, 2),\n",
    "        \"out_true_peak_dbTP\": round(out_tp, 2),\n",
    "        \"lufs_error_db\": round(out_lufs - profile.target_lufs, 2)  # non-zero if we trimmed for TP\n",
    "    }\n",
    "    return y, meta\n",
    "\n",
    "# ---- Batch runner + export ----\n",
    "def simulate_and_export_for_platforms(\n",
    "    input_path: str,\n",
    "    out_dir: str,\n",
    "    profiles: Optional[Dict[str, StreamingProfile]] = None,\n",
    "    bit_depth: str = \"PCM_24\",\n",
    "    register_to_manifest: Optional[tuple] = None,  # (manifest, kind_str)\n",
    ") -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate 'as-heard' files for multiple platforms from a pre-master or master.\n",
    "    Returns (list_of_paths, summary_dataframe).\n",
    "    \"\"\"\n",
    "    profiles = profiles or default_streaming_profiles()\n",
    "\n",
    "    # read\n",
    "    x, sr = sf.read(input_path)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    out_paths = []\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "\n",
    "    for name, prof in profiles.items():\n",
    "        y, meta = simulate_streaming_as_heard(x, sr, prof)\n",
    "        out_name = f\"{base}__asheard_{name}.wav\"\n",
    "        out_path = os.path.join(out_dir, out_name)\n",
    "        sf.write(out_path, y, sr, subtype=bit_depth)\n",
    "        out_paths.append(out_path)\n",
    "\n",
    "        row = {\n",
    "            \"platform\": name,\n",
    "            \"target_lufs\": prof.target_lufs,\n",
    "            \"tp_ceiling_db\": prof.tp_ceiling_db,\n",
    "            \"tp_strategy\": prof.tp_strategy,\n",
    "            **{k: meta[k] for k in [\"in_lufs\",\"in_true_peak_dbTP\",\"gain_to_target_db\",\"tp_action\",\"extra_trim_db\",\"out_lufs\",\"out_true_peak_dbTP\",\"lufs_error_db\"]},\n",
    "            \"asheard_path\": out_path,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        # optional manifest registration\n",
    "        if register_to_manifest is not None:\n",
    "            man, kind = register_to_manifest\n",
    "            register_artifact(man, out_path, kind=kind, params={\"profile\": name, **meta}, stage=f\"asheard_{name}\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return out_paths, df\n",
    "\n",
    "def print_streaming_summary(df: pd.DataFrame):\n",
    "    cols = [\n",
    "        \"platform\",\"target_lufs\",\"tp_ceiling_db\",\n",
    "        \"in_lufs\",\"gain_to_target_db\",\"tp_action\",\"extra_trim_db\",\n",
    "        \"out_lufs\",\"lufs_error_db\",\"out_true_peak_dbTP\"\n",
    "    ]\n",
    "    if set(cols).issubset(df.columns):\n",
    "        print(df[cols].to_string(index=False))\n",
    "    else:\n",
    "        print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6bd99-396c-496d-87fb-d7a2ce4f1467",
   "metadata": {},
   "source": [
    "### Comparison & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71303251-e060-495f-bc08-01631552746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison & Reporting layer loaded:\n",
      "- collect_metrics, build_comparison_tables\n",
      "- plot_overlays (saves to reports/)\n",
      "- make_blind_ab_pack (optional)\n",
      "- write_report_html (self-contained)\n",
      "- write_report_bundle (one-shot with manifest)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Comparison & Reporting — Notebook Layer\n",
    "# ============================================\n",
    "# What this provides:\n",
    "# - collect_metrics(paths): one-call metrics for many files (uses your Analysis layer)\n",
    "# - build_comparison_tables(...): summary + deltas vs reference\n",
    "# - plot_overlays(...): spectrum + short-term loudness overlays (saved to reports/)\n",
    "# - make_blind_ab_pack(...): copies/renames files for unbiased listening\n",
    "# - write_report_html(...): self-contained HTML report with tables + plots\n",
    "# - write_report_bundle(...): one-shot wrapper that does all of the above\n",
    "#\n",
    "# Notes:\n",
    "# - No code is executed on import; you’ll call these functions later.\n",
    "# - Uses only matplotlib (no seaborn), and saves figures (no blocking .show()).\n",
    "# - Registers generated artifacts in your manifest when you pass it in.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import os, io, shutil, warnings, json, uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 110\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "try:\n",
    "    from scipy.io.wavfile import WavFileWarning\n",
    "except ImportError:\n",
    "    class WavFileWarning(UserWarning): pass\n",
    "warnings.filterwarnings(\"ignore\", category=WavFileWarning)\n",
    "\n",
    "# ---------- Config dataclasses ----------\n",
    "\n",
    "@dataclass\n",
    "class CompareConfig:\n",
    "    preview_seconds: int = 60\n",
    "    nfft: int = 1 << 16     # spectrum segment size\n",
    "    win_s: float = 3.0      # short-term loudness window\n",
    "    hop_s: float = 0.5\n",
    "    level_match_mode: str = \"none\"   # \"none\" | \"ref_lufs\" | \"target_lufs\"\n",
    "    target_lufs: float = -14.0       # used when level_match_mode == \"target_lufs\"\n",
    "    reference_name: Optional[str] = None  # which row is the reference for Δ tables\n",
    "\n",
    "# ---------- Internals: fast audio readers & helpers ----------\n",
    "\n",
    "def _read_mono_preview(path: str, preview_seconds: Optional[int]) -> Tuple[int, np.ndarray]:\n",
    "    sr0, data0 = wavfile.read(path)\n",
    "    if data0.dtype == np.int16:\n",
    "        x0 = data0.astype(np.float32) / 32768.0\n",
    "    elif data0.dtype == np.int32:\n",
    "        x0 = data0.astype(np.float32) / 2147483648.0\n",
    "    elif data0.dtype == np.uint8:\n",
    "        x0 = (data0.astype(np.float32) - 128.0) / 128.0\n",
    "    else:\n",
    "        x0 = data0.astype(np.float32)\n",
    "    mono = x0 if x0.ndim == 1 else np.mean(x0, axis=1)\n",
    "    if preview_seconds is not None:\n",
    "        n = int(min(len(mono), preview_seconds * sr0))\n",
    "        mono = mono[:n]\n",
    "    return sr0, mono\n",
    "\n",
    "def _spectrum_xy(mono: np.ndarray, sr: int, nfft: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    seg = mono[:min(len(mono), nfft)]\n",
    "    if len(seg) < 2048:\n",
    "        pad = np.zeros(2048, dtype=seg.dtype); pad[:len(seg)] = seg; seg = pad\n",
    "    win = np.hanning(len(seg))\n",
    "    sp = np.fft.rfft(seg * win)\n",
    "    freqs = np.fft.rfftfreq(len(seg), 1/sr)\n",
    "    mag_db = 20*np.log10(np.maximum(1e-12, np.abs(sp)))\n",
    "    return freqs, mag_db\n",
    "\n",
    "def _short_term_loudness(mono: np.ndarray, sr: int, win_s: float, hop_s: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    win = int(max(2, round(win_s * sr)))\n",
    "    hop = int(max(1, round(hop_s * sr)))\n",
    "    kernel = np.ones(win, dtype=np.float32) / float(win)\n",
    "    pow_sig = mono**2\n",
    "    rms = np.sqrt(np.maximum(1e-20, np.convolve(pow_sig, kernel, mode=\"same\")))\n",
    "    idx = np.arange(0, len(mono), hop)\n",
    "    t = idx / sr\n",
    "    return t, 20*np.log10(np.maximum(1e-12, rms[idx]))\n",
    "\n",
    "def _lufs_approx(x: np.ndarray, sr: int) -> float:\n",
    "    # Uses your Analysis layer function if available\n",
    "    try:\n",
    "        return lufs_integrated_approx(x, sr)\n",
    "    except NameError:\n",
    "        # tiny inline fallback (not gated)\n",
    "        from scipy import signal\n",
    "        sos_hp = signal.butter(2, 38.0/(sr*0.5), btype='highpass', output='sos')\n",
    "        mono = x if x.ndim == 1 else np.mean(x, axis=1)\n",
    "        y = signal.sosfilt(sos_hp, mono)\n",
    "        ms = float(np.mean(y**2))\n",
    "        return -0.691 + 10.0*np.log10(max(1e-12, ms))\n",
    "\n",
    "def _tp_approx_db(x: np.ndarray, sr: int, oversample: int = 4) -> float:\n",
    "    try:\n",
    "        return true_peak_dbfs(x, sr, oversample=oversample)\n",
    "    except NameError:\n",
    "        from scipy import signal\n",
    "        x_os = signal.resample_poly(x, oversample, 1, axis=0 if x.ndim>1 else 0)\n",
    "        tp = float(np.max(np.abs(x_os)))\n",
    "        return 20.0*np.log10(max(1e-12, tp))\n",
    "\n",
    "# ---------- 1) Metrics collection ----------\n",
    "\n",
    "def collect_metrics(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each path, run your Analysis layer and collect key metrics.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for p in file_paths:\n",
    "        rep = analyze_wav(p)  # uses your Analysis layer\n",
    "        rows.append({\n",
    "            \"name\": os.path.splitext(os.path.basename(p))[0],\n",
    "            \"path\": os.path.abspath(p),\n",
    "            \"sr\": rep.sr,\n",
    "            \"duration_s\": rep.duration_s,\n",
    "            \"peak_dbfs\": rep.basic[\"peak_dbfs\"],\n",
    "            \"true_peak_dbfs\": rep.true_peak_dbfs,\n",
    "            \"rms_dbfs\": rep.basic[\"rms_dbfs\"],\n",
    "            \"lufs_int\": rep.lufs_integrated,\n",
    "            \"crest_db\": rep.basic[\"crest_db\"],\n",
    "            \"bass_%\": rep.bass_energy_pct,\n",
    "            \"air_%\": rep.air_energy_pct,\n",
    "            \"phase_corr\": rep.stereo[\"phase_correlation\"],\n",
    "            \"stereo_width\": rep.stereo[\"stereo_width\"],\n",
    "            \"spectral_flatness\": rep.spectral_flatness,\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# ---------- 2) Build comparison tables (summary + deltas) ----------\n",
    "\n",
    "def build_comparison_tables(df_metrics: pd.DataFrame, cfg: CompareConfig) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Returns (summary_df, deltas_df_vs_reference|None).\n",
    "    \"\"\"\n",
    "    summary = df_metrics.copy()\n",
    "\n",
    "    deltas = None\n",
    "    ref_name = cfg.reference_name or (summary.iloc[0][\"name\"] if len(summary) else None)\n",
    "    if ref_name and ref_name in list(summary[\"name\"]):\n",
    "        ref_row = summary[summary[\"name\"] == ref_name].iloc[0]\n",
    "        # metrics to delta\n",
    "        delta_cols = [\"peak_dbfs\",\"true_peak_dbfs\",\"rms_dbfs\",\"lufs_int\",\"crest_db\",\"bass_%\",\"air_%\",\"phase_corr\",\"stereo_width\",\"spectral_flatness\"]\n",
    "        rows = []\n",
    "        for _, r in summary.iterrows():\n",
    "            d = { \"name\": r[\"name\"] }\n",
    "            for c in delta_cols:\n",
    "                d[f\"Δ {c}\"] = float(r[c] - ref_row[c])\n",
    "            rows.append(d)\n",
    "        deltas = pd.DataFrame(rows)\n",
    "    return summary, deltas\n",
    "\n",
    "# ---------- 3) Plots: spectrum & loudness overlays ----------\n",
    "# Saves files under reports_dir and returns list of paths\n",
    "\n",
    "def plot_overlays(file_paths: List[str], labels: Optional[List[str]], reports_dir: str, cfg: CompareConfig) -> Dict[str, str]:\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    labels = labels or [os.path.splitext(os.path.basename(p))[0] for p in file_paths]\n",
    "\n",
    "    # Spectrum\n",
    "    plt.figure()\n",
    "    for p, lbl in zip(file_paths, labels):\n",
    "        sr, mono = _read_mono_preview(p, cfg.preview_seconds)\n",
    "        f, m = _spectrum_xy(mono, sr, cfg.nfft)\n",
    "        plt.plot(f, m, label=lbl)\n",
    "    plt.xscale('log'); plt.xlim(20, 20000)\n",
    "    plt.xlabel(\"Frequency (Hz)\"); plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.title(f\"Spectrum Overlay (first {cfg.preview_seconds}s)\")\n",
    "    plt.legend()\n",
    "    spec_png = os.path.join(reports_dir, \"spectrum_overlay.png\")\n",
    "    plt.savefig(spec_png, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Short-term loudness\n",
    "    plt.figure()\n",
    "    series, min_len = [], None\n",
    "    for p, lbl in zip(file_paths, labels):\n",
    "        sr, mono = _read_mono_preview(p, cfg.preview_seconds)\n",
    "        t, s = _short_term_loudness(mono, sr, cfg.win_s, cfg.hop_s)\n",
    "        series.append((t, s, lbl))\n",
    "        min_len = len(s) if min_len is None else min(min_len, len(s))\n",
    "    for t, s, lbl in series:\n",
    "        plt.plot(t[:min_len], s[:min_len], label=lbl)\n",
    "    plt.xlabel(\"Time (s)\"); plt.ylabel(\"Short-term RMS (dBFS)\")\n",
    "    plt.title(f\"Short-term Loudness Overlay (first {cfg.preview_seconds}s)\")\n",
    "    plt.legend()\n",
    "    loud_png = os.path.join(reports_dir, \"loudness_overlay.png\")\n",
    "    plt.savefig(loud_png, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    return {\"spectrum_png\": spec_png, \"loudness_png\": loud_png}\n",
    "\n",
    "# ---------- 4) Blind A/B pack (optional) ----------\n",
    "\n",
    "def make_blind_ab_pack(file_paths: List[str], out_dir: str, bit_depth: str = \"PCM_24\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Copies files with randomized neutral labels (A/B/C...), returns the new paths.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    labels = [chr(ord('A') + i) for i in range(len(file_paths))]\n",
    "    order = np.arange(len(file_paths))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    out_paths = []\n",
    "    for idx, new_lbl in zip(order, labels):\n",
    "        src = file_paths[idx]\n",
    "        x, sr = sf.read(src)\n",
    "        dst = os.path.join(out_dir, f\"Blind_{new_lbl}.wav\")\n",
    "        sf.write(dst, x, sr, subtype=bit_depth)\n",
    "        out_paths.append(dst)\n",
    "    return out_paths\n",
    "\n",
    "# ---------- 5) HTML Report ----------\n",
    "\n",
    "def _df_to_html_table(df: pd.DataFrame, caption: str) -> str:\n",
    "    buf = io.StringIO()\n",
    "    buf.write(f\"<h3>{caption}</h3>\\n\")\n",
    "    buf.write(df.to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "    return buf.getvalue()\n",
    "\n",
    "def write_report_html(\n",
    "    summary_df: pd.DataFrame,\n",
    "    deltas_df: Optional[pd.DataFrame],\n",
    "    plots: Dict[str, str],\n",
    "    out_path: str,\n",
    "    title: str = \"Post-Mix Comparison Report\",\n",
    "    extra_notes: Optional[str] = None\n",
    ") -> str:\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_path)), exist_ok=True)\n",
    "    html = io.StringIO()\n",
    "    html.write(f\"<!doctype html><html><head><meta charset='utf-8'><title>{title}</title>\")\n",
    "    html.write(\"<style>body{font-family:system-ui,Arial,sans-serif;margin:24px} h1{margin-top:0} img{max-width:100%;height:auto} table{border-collapse:collapse} th,td{border:1px solid #ddd;padding:6px} caption{margin:6px 0}</style>\")\n",
    "    html.write(\"</head><body>\")\n",
    "    html.write(f\"<h1>{title}</h1>\")\n",
    "    if extra_notes:\n",
    "        html.write(f\"<p><em>{extra_notes}</em></p>\")\n",
    "    # tables\n",
    "    html.write(_df_to_html_table(summary_df, \"Summary Metrics\"))\n",
    "    if deltas_df is not None and len(deltas_df):\n",
    "        html.write(_df_to_html_table(deltas_df, \"Δ vs Reference\"))\n",
    "    # plots\n",
    "    if \"spectrum_png\" in plots and os.path.exists(plots[\"spectrum_png\"]):\n",
    "        html.write(\"<h3>Spectrum Overlay</h3>\")\n",
    "        html.write(f\"<img src='{os.path.basename(plots['spectrum_png'])}' alt='Spectrum Overlay'/>\")\n",
    "    if \"loudness_png\" in plots and os.path.exists(plots[\"loudness_png\"]):\n",
    "        html.write(\"<h3>Short-Term Loudness Overlay</h3>\")\n",
    "        html.write(f\"<img src='{os.path.basename(plots['loudness_png'])}' alt='Loudness Overlay'/>\")\n",
    "    html.write(\"</body></html>\")\n",
    "\n",
    "    # write HTML and copy plot assets next to it\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html.getvalue())\n",
    "    # copy images to same folder for portability\n",
    "    for k, p in plots.items():\n",
    "        if os.path.exists(p):\n",
    "            shutil.copy2(p, os.path.join(os.path.dirname(out_path), os.path.basename(p)))\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "# ---------- 6) One-shot bundle: metrics → plots → HTML (+ manifest) ----------\n",
    "\n",
    "def write_report_bundle(\n",
    "    file_paths: List[str],\n",
    "    reports_dir: str,\n",
    "    cfg: Optional[CompareConfig] = None,\n",
    "    manifest: Optional[Any] = None,   # pass your Manifest object to auto-register\n",
    "    report_name: str = \"comparison_report.html\",\n",
    "    extra_notes: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper to:\n",
    "      - compute metrics for all files\n",
    "      - build summary + deltas (vs reference)\n",
    "      - render plots (saved)\n",
    "      - write HTML report\n",
    "      - register artifacts in manifest (optional)\n",
    "    Returns a dict with paths and DataFrames.\n",
    "    \"\"\"\n",
    "    cfg = cfg or CompareConfig()\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "    # 1) metrics\n",
    "    df_metrics = collect_metrics(file_paths)\n",
    "    # 2) tables\n",
    "    summary_df, deltas_df = build_comparison_tables(df_metrics, cfg)\n",
    "    # 3) plots\n",
    "    labels = list(summary_df[\"name\"])\n",
    "    plot_paths = plot_overlays(file_paths, labels, reports_dir, cfg)\n",
    "    # 4) HTML\n",
    "    html_path = os.path.join(reports_dir, report_name)\n",
    "    html_path = write_report_html(summary_df, deltas_df, plot_paths, html_path, extra_notes=extra_notes)\n",
    "\n",
    "    # 5) optional manifest registration (HTML + images as a single \"report\" artifact)\n",
    "    if manifest is not None:\n",
    "        register_artifact(manifest, html_path, kind=\"report\", params={\n",
    "            \"type\": \"comparison\",\n",
    "            \"reference\": cfg.reference_name,\n",
    "            \"preview_seconds\": cfg.preview_seconds,\n",
    "        }, stage=\"compare_html\")\n",
    "        for p in plot_paths.values():\n",
    "            if os.path.exists(p):\n",
    "                register_artifact(manifest, p, kind=\"report_asset\", params={\"linked_report\": os.path.basename(html_path)})\n",
    "\n",
    "    return {\n",
    "        \"html_path\": html_path,\n",
    "        \"summary_df\": summary_df,\n",
    "        \"deltas_df\": deltas_df,\n",
    "        \"plots\": plot_paths\n",
    "    }\n",
    "\n",
    "print(\"Comparison & Reporting layer loaded:\")\n",
    "print(\"- collect_metrics, build_comparison_tables\")\n",
    "print(\"- plot_overlays (saves to reports/)\")\n",
    "print(\"- make_blind_ab_pack (optional)\")\n",
    "print(\"- write_report_html (self-contained)\")\n",
    "print(\"- write_report_bundle (one-shot with manifest)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2cd1f-6366-4a8e-b5c4-e1df74b61408",
   "metadata": {},
   "source": [
    "### Presets & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee2a35a1-2c19-4baf-a8b4-9b7840834ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presets & Recommendations layer loaded:\n",
      "- PRESETS dict, list_presets(), get_preset(name)\n",
      "- recommend_from_analysis(rep) → [Recommendation]\n",
      "- build_premaster_plan_from_recs(recs, limit)\n",
      "- recommend_mastering_styles_from_metrics(rep)\n",
      "- recommendation_summary(recs)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Presets & Recommendations — Notebook Layer\n",
    "# ============================================\n",
    "# Provides:\n",
    "# - DialState presets (named, ready to render)\n",
    "# - Rule-based recommender from analysis metrics → dial suggestions\n",
    "# - Plan builders for batch rendering with RenderEngine / Orchestrator\n",
    "#\n",
    "# Nothing executes on import; you’ll call funcs later.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Reuse the DialState from Render Engine if already defined; else define a tiny fallback.\n",
    "try:\n",
    "    DialState\n",
    "except NameError:\n",
    "    @dataclass\n",
    "    class DialState:\n",
    "        bass: float = 0.0     # 0..100\n",
    "        punch: float = 0.0    # 0..100\n",
    "        clarity: float = 0.0  # 0..100\n",
    "        air: float = 0.0      # 0..100\n",
    "        width: float = 0.0    # 0..100\n",
    "\n",
    "# ---------- 1) Preset Library (you can tweak these) ----------\n",
    "\n",
    "PRESETS: Dict[str, DialState] = {\n",
    "    # Gentle, safe polish\n",
    "    \"Balanced Gentle\":   DialState(bass=15, punch=15, clarity=10, air=10, width=5),\n",
    "    # Bass-forward modern pop/hip-hop\n",
    "    \"Modern Low-End\":    DialState(bass=38, punch=28, clarity=8,  air=8,  width=8),\n",
    "    # Tight low end for dance/electronic\n",
    "    \"Tight & Punchy\":    DialState(bass=28, punch=42, clarity=12, air=10, width=6),\n",
    "    # De-mud + sparkle for dense guitars/vocals\n",
    "    \"Clarity & Air\":     DialState(bass=10, punch=12, clarity=28, air=28, width=6),\n",
    "    # Wider image, modest tone moves\n",
    "    \"Wide Pop\":          DialState(bass=18, punch=14, clarity=10, air=16, width=18),\n",
    "    # Minimal changes (QA/reference)\n",
    "    \"Transparent\":       DialState(bass=0,  punch=0,  clarity=0,  air=0,  width=0),\n",
    "}\n",
    "\n",
    "def list_presets() -> List[str]:\n",
    "    return list(PRESETS.keys())\n",
    "\n",
    "def get_preset(name: str) -> DialState:\n",
    "    return PRESETS[name]\n",
    "\n",
    "# ---------- 2) Metric-driven Recommendations ----------\n",
    "\n",
    "@dataclass\n",
    "class Recommendation:\n",
    "    name: str                       # a label for this suggestion\n",
    "    dials: DialState                # suggested dial positions (0..100)\n",
    "    priority: int                   # lower = earlier to try\n",
    "    rationale: List[str]            # bullet points (human-readable “why”)\n",
    "    notes: Optional[str] = None     # extra context\n",
    "\n",
    "def _clip01(x: float, lo=0.0, hi=100.0) -> float:\n",
    "    return float(max(lo, min(hi, x)))\n",
    "\n",
    "def _scale(val: float, in_lo: float, in_hi: float, out_lo: float, out_hi: float) -> float:\n",
    "    # linear map with clamping\n",
    "    if in_hi == in_lo:\n",
    "        return out_lo\n",
    "    t = (val - in_lo) / (in_hi - in_lo)\n",
    "    t = max(0.0, min(1.0, t))\n",
    "    return out_lo + t * (out_hi - out_lo)\n",
    "\n",
    "def recommend_from_analysis(rep) -> List[Recommendation]:\n",
    "    \"\"\"\n",
    "    Takes an AnalysisReport (from analyze_wav) OR a dict with the same fields we need.\n",
    "    Returns a sorted list of Recommendation objects.\n",
    "    \"\"\"\n",
    "    # Extract metrics in a defensive way\n",
    "    def _get(dct, k, default=0.0):\n",
    "        return float(dct.get(k, default))\n",
    "\n",
    "    # normalize input\n",
    "    if hasattr(rep, \"bass_energy_pct\"):\n",
    "        # It's an AnalysisReport\n",
    "        basic = rep.basic\n",
    "        stereo = rep.stereo\n",
    "        lufs = rep.lufs_integrated\n",
    "        tp   = rep.true_peak_dbfs\n",
    "        bass = rep.bass_energy_pct\n",
    "        airp = rep.air_energy_pct\n",
    "        flat = rep.spectral_flatness\n",
    "    else:\n",
    "        # Expect a dict-like structure\n",
    "        basic = rep.get(\"basic\", rep)\n",
    "        stereo = rep.get(\"stereo\", rep)\n",
    "        lufs = _get(rep, \"lufs_integrated\", -20.0)\n",
    "        tp   = _get(rep, \"true_peak_dbfs\", -1.5)\n",
    "        bass = _get(rep, \"bass_energy_pct\", 40.0)\n",
    "        airp = _get(rep, \"air_energy_pct\", 0.2)\n",
    "        flat = _get(rep, \"spectral_flatness\", 0.05)\n",
    "\n",
    "    peak_dbfs = _get(basic, \"peak_dbfs\", -3.0)\n",
    "    rms_dbfs  = _get(basic, \"rms_dbfs\", -20.0)\n",
    "    crest_db  = _get(basic, \"crest_db\", 12.0)\n",
    "    phase_corr = _get(stereo, \"phase_correlation\", 0.5)\n",
    "    width      = _get(stereo, \"stereo_width\", 0.4)\n",
    "\n",
    "    recs: List[Recommendation] = []\n",
    "\n",
    "    # --- Heuristics (tunable) ---\n",
    "    # Tonal balance\n",
    "    very_bassy  = bass > 65.0\n",
    "    super_bassy = bass > 80.0\n",
    "    very_dark   = airp < 0.02\n",
    "    dark        = airp < 0.1\n",
    "    brightish   = airp > 0.7\n",
    "\n",
    "    # Loudness/dynamics context\n",
    "    very_dynamic = crest_db > 16\n",
    "    squashed     = crest_db < 8\n",
    "\n",
    "    # Stereo context\n",
    "    too_narrow   = width < 0.35\n",
    "    very_wide    = phase_corr < 0.2\n",
    "\n",
    "    #################################################################\n",
    "    # 1) Fix dark + bass-heavy → more Air, some Clarity, moderate Punch\n",
    "    #################################################################\n",
    "    if very_bassy and (dark or very_dark):\n",
    "        air_amt = 25 if very_dark else 18\n",
    "        clar_amt = 18 if super_bassy else 12\n",
    "        punch_amt = 30 if very_dynamic else 22\n",
    "        dials = DialState(\n",
    "            bass= max(10, _scale(bass, 60, 90, 14, 28)),  # keep some bass but don’t add more\n",
    "            punch= punch_amt,\n",
    "            clarity= clar_amt,\n",
    "            air= air_amt,\n",
    "            width= 8 if too_narrow else 6\n",
    "        )\n",
    "        recs.append(Recommendation(\n",
    "            name=\"De-mud & Open Top\",\n",
    "            dials=dials,\n",
    "            priority=1,\n",
    "            rationale=[\n",
    "                f\"bass_energy %{bass:.1f} is high → reduce mud via clarity + keep lows controlled\",\n",
    "                f\"air_energy %{airp:.3f} is low → add air shelf for brightness\",\n",
    "                f\"crest {crest_db:.1f} dB → moderate punch to emphasize transients\",\n",
    "            ],\n",
    "            notes=\"Start here if the mix feels boomy/dull.\"\n",
    "        ))\n",
    "\n",
    "    #################################################################\n",
    "    # 2) If very dynamic + quiet → add Punch, little Bass, gentle Air\n",
    "    #################################################################\n",
    "    if very_dynamic and lufs < -18:\n",
    "        dials = DialState(\n",
    "            bass= 16,\n",
    "            punch= 38,\n",
    "            clarity= 10,\n",
    "            air= 14,\n",
    "            width= 8 if too_narrow else 6\n",
    "        )\n",
    "        recs.append(Recommendation(\n",
    "            name=\"Punch Up the Transients\",\n",
    "            dials=dials,\n",
    "            priority=2,\n",
    "            rationale=[\n",
    "                f\"crest {crest_db:.1f} dB (dynamic) and LUFS {lufs:.1f} (quiet) → add transient definition\",\n",
    "                \"small air lift to help intelligibility\",\n",
    "            ]\n",
    "        ))\n",
    "\n",
    "    #################################################################\n",
    "    # 3) If midrange congested (not dark, not bright, but high flatness) → Clarity focus\n",
    "    #################################################################\n",
    "    if 0.03 < flat < 0.12 and not dark and not brightish:\n",
    "        dials = DialState(\n",
    "            bass= 10,\n",
    "            punch= 18,\n",
    "            clarity= 26,\n",
    "            air= 10,\n",
    "            width= 8\n",
    "        )\n",
    "        recs.append(Recommendation(\n",
    "            name=\"Clear Midrange\",\n",
    "            dials=dials,\n",
    "            priority=3,\n",
    "            rationale=[\n",
    "                f\"spectral_flatness {flat:.3f} suggests dense content → de-mud around 180–230 Hz\",\n",
    "                \"moderate punch for definition without aggression\",\n",
    "            ]\n",
    "        ))\n",
    "\n",
    "    #################################################################\n",
    "    # 4) If image is narrow → widen (safely)\n",
    "    #################################################################\n",
    "    if too_narrow:\n",
    "        dials = DialState(\n",
    "            bass= 14,\n",
    "            punch= 14,\n",
    "            clarity= 10,\n",
    "            air= 12,\n",
    "            width= _scale(width, 0.2, 0.4, 12, 22)\n",
    "        )\n",
    "        recs.append(Recommendation(\n",
    "            name=\"Widen Image (Safe)\",\n",
    "            dials=dials,\n",
    "            priority=4,\n",
    "            rationale=[\n",
    "                f\"stereo_width {width:.2f} is narrow → add width\",\n",
    "                \"tone moves kept gentle to avoid destabilizing center\"\n",
    "            ],\n",
    "            notes=\"If mono compatibility is critical, keep width ≤ 15.\"\n",
    "        ))\n",
    "\n",
    "    #################################################################\n",
    "    # 5) Default safe polish if no strong issues\n",
    "    #################################################################\n",
    "    if not recs:\n",
    "        recs.append(Recommendation(\n",
    "            name=\"Balanced Gentle\",\n",
    "            dials=PRESETS[\"Balanced Gentle\"],\n",
    "            priority=9,\n",
    "            rationale=[\"No strong issues detected → start with light, broad polish.\"]\n",
    "        ))\n",
    "\n",
    "    # Stable ordering\n",
    "    recs.sort(key=lambda r: r.priority)\n",
    "    return recs\n",
    "\n",
    "# ---------- 3) Render/Orchestrator Plans (without executing) ----------\n",
    "\n",
    "def build_premaster_plan_from_recs(\n",
    "    recs: List[Recommendation],\n",
    "    limit: int = 3,\n",
    "    prefix: str = \"PM\"\n",
    ") -> List[Tuple[str, DialState]]:\n",
    "    \"\"\"\n",
    "    Convert top-N recommendations to (name, DialState) tuples for RenderEngine.commit_variants().\n",
    "    \"\"\"\n",
    "    planned: List[Tuple[str, DialState]] = []\n",
    "    for i, r in enumerate(recs[:limit], start=1):\n",
    "        tag = f\"{prefix}{i}_{r.name.replace(' ', '')}\"\n",
    "        planned.append((tag, r.dials))\n",
    "    return planned\n",
    "\n",
    "def recommend_mastering_styles_from_metrics(rep) -> List[Tuple[str, float, str]]:\n",
    "    \"\"\"\n",
    "    Suggest LocalMasterProvider styles from analysis:\n",
    "    Returns list of tuples: (style, strength 0..1, why)\n",
    "    \"\"\"\n",
    "    lufs = getattr(rep, \"lufs_integrated\", -20.0)\n",
    "    crest = rep.basic[\"crest_db\"] if hasattr(rep, \"basic\") else rep.get(\"crest_db\", 12.0)\n",
    "    airp  = getattr(rep, \"air_energy_pct\", 0.2)\n",
    "    bass  = getattr(rep, \"bass_energy_pct\", 40.0)\n",
    "\n",
    "    out: List[Tuple[str,float,str]] = []\n",
    "    # If dark → try \"bright\"\n",
    "    if airp < 0.05:\n",
    "        out.append((\"bright\", 0.6, f\"Low air ({airp:.3f}) → add sheen\"))\n",
    "    # If bass-heavy → try \"neutral\" vs \"warm\" (depending on taste)\n",
    "    if bass > 70:\n",
    "        out.append((\"neutral\", 0.5, f\"High bass energy ({bass:.1f}%) → keep low-end in check\"))\n",
    "    else:\n",
    "        out.append((\"warm\", 0.5, f\"Moderate bass ({bass:.1f}%) → touch of weight\"))\n",
    "    # If very dynamic & quiet → try \"loud\" moderately\n",
    "    if crest > 16 and lufs < -18:\n",
    "        out.append((\"loud\", 0.55, f\"Dynamic ({crest:.1f} dB) & quiet ({lufs:.1f} LUFS) → more forward\"))\n",
    "\n",
    "    # Always keep a neutral baseline\n",
    "    if not any(s == \"neutral\" for s,_,_ in out):\n",
    "        out.insert(0, (\"neutral\", 0.5, \"Baseline reference\"))\n",
    "\n",
    "    # Deduplicate, preserve order\n",
    "    seen=set(); filtered=[]\n",
    "    for s in out:\n",
    "        if s[0] in seen: continue\n",
    "        seen.add(s[0]); filtered.append(s)\n",
    "    return filtered\n",
    "\n",
    "# ---------- 4) Human-readable summary helpers ----------\n",
    "\n",
    "def recommendation_summary(recs: List[Recommendation]) -> str:\n",
    "    lines = []\n",
    "    for r in recs:\n",
    "        lines.append(f\"- {r.name} (priority {r.priority}): \"\n",
    "                     f\"B{r.dials.bass:.0f} P{r.dials.punch:.0f} C{r.dials.clarity:.0f} A{r.dials.air:.0f} W{r.dials.width:.0f}\")\n",
    "        for why in r.rationale:\n",
    "            lines.append(f\"    • {why}\")\n",
    "        if r.notes:\n",
    "            lines.append(f\"    ↪ {r.notes}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"Presets & Recommendations layer loaded:\")\n",
    "print(\"- PRESETS dict, list_presets(), get_preset(name)\")\n",
    "print(\"- recommend_from_analysis(rep) → [Recommendation]\")\n",
    "print(\"- build_premaster_plan_from_recs(recs, limit)\")\n",
    "print(\"- recommend_mastering_styles_from_metrics(rep)\")\n",
    "print(\"- recommendation_summary(recs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1811c76-2e44-4403-a2d1-7c8ebd35d62b",
   "metadata": {},
   "source": [
    "### Logging, Versioning, Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed3ef976-aad6-4a82-b1c6-42b23e79c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging · Versioning · Reproducibility layer loaded:\n",
      "- RunLogger.start(workspace_root, tag) → logger\n",
      "- logger.log_params/metrics/artifact(), logger.write_summary()\n",
      "- capture_environment(), file_sha256(), capture_config_snapshot()\n",
      "- processing_digest(name, code_versions, params)\n",
      "- make_repro_zip(out_zip, workspace_root, logger, env_info, inputs, outputs)\n",
      "- register_and_log_artifact(manifest, logger, path, kind, params, stage)\n",
      "- CODE_VERSIONS dict (update per layer when you change code)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Logging · Versioning · Reproducibility Layer\n",
    "# ============================================\n",
    "# What this provides:\n",
    "# - RunLogger: creates a run_id and writes structured logs (JSONL) + summary JSON\n",
    "# - Env capture: library versions, Python/OS, CPU info, pip freeze snapshot\n",
    "# - Determinism: simple seed manager for NumPy & Python hash seed\n",
    "# - Provenance: content hashes for audio/code/config; processing graph digest\n",
    "# - Artifact registry helpers (alongside your Manifest)\n",
    "# - Repro bundle: packs config, logs, environment, and outputs into a zip\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Optional, List, Iterable, Tuple\n",
    "import os, sys, io, json, time, uuid, hashlib, platform, subprocess, zipfile, shutil, textwrap\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# ---------- small utils ----------\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "\n",
    "def _mkdirp(p: str) -> str:\n",
    "    os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "def _sha256_bytes(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def file_sha256(path: str, bufsize: int = 1<<20) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(bufsize), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def json_sha256(obj: Any) -> str:\n",
    "    \"\"\"Stable JSON hash (sorted keys, no whitespace).\"\"\"\n",
    "    b = json.dumps(obj, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n",
    "    return _sha256_bytes(b)\n",
    "\n",
    "# ---------- determinism ----------\n",
    "\n",
    "class SeedScope:\n",
    "    \"\"\"\n",
    "    Context manager to set deterministic seeds for NumPy and PYTHONHASHSEED.\n",
    "    Use: with SeedScope(42): ...  (or call SeedScope.set_global(42))\n",
    "    \"\"\"\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.seed = int(seed)\n",
    "        self._prev_hashseed = os.environ.get(\"PYTHONHASHSEED\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        np.random.seed(self.seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(self.seed)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if self._prev_hashseed is None:\n",
    "            os.environ.pop(\"PYTHONHASHSEED\", None)\n",
    "        else:\n",
    "            os.environ[\"PYTHONHASHSEED\"] = self._prev_hashseed\n",
    "\n",
    "    @staticmethod\n",
    "    def set_global(seed: int = 42):\n",
    "        np.random.seed(int(seed))\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(int(seed))\n",
    "\n",
    "# ---------- environment capture ----------\n",
    "\n",
    "def capture_environment() -> Dict[str, Any]:\n",
    "    info = {\n",
    "        \"timestamp\": _now_iso(),\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"platform\": {\n",
    "            \"system\": platform.system(),\n",
    "            \"release\": platform.release(),\n",
    "            \"version\": platform.version(),\n",
    "            \"machine\": platform.machine(),\n",
    "            \"processor\": platform.processor(),\n",
    "        },\n",
    "        \"packages\": {},\n",
    "        \"pip_freeze\": None,\n",
    "    }\n",
    "    # library versions (best-effort)\n",
    "    try:\n",
    "        import scipy, soundfile, pandas, matplotlib\n",
    "        info[\"packages\"].update({\n",
    "            \"numpy\": np.__version__,\n",
    "            \"scipy\": scipy.__version__,\n",
    "            \"soundfile\": soundfile.__version__,\n",
    "            \"pandas\": pandas.__version__,\n",
    "            \"matplotlib\": matplotlib.__version__,\n",
    "        })\n",
    "    except Exception:\n",
    "        info[\"packages\"][\"numpy\"] = np.__version__\n",
    "    # pip freeze (optional, can be heavy)\n",
    "    try:\n",
    "        out = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], timeout=20)\n",
    "        info[\"pip_freeze\"] = out.decode(\"utf-8\").splitlines()\n",
    "    except Exception:\n",
    "        info[\"pip_freeze\"] = None\n",
    "    return info\n",
    "\n",
    "# ---------- processing graph digest ----------\n",
    "\n",
    "def processing_digest(name: str, *, code_versions: Dict[str, str], params: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Create a short fingerprint for a processing step: hashes code + params.\n",
    "    code_versions: e.g., {\"dsp_primitives\":\"v1.2.0+sha...\", \"processors\":\"v0.9.3\", ...}\n",
    "    params: the dial values, thresholds, etc.\n",
    "    \"\"\"\n",
    "    payload = {\"step\": name, \"code\": code_versions, \"params\": params}\n",
    "    h = json_sha256(payload)\n",
    "    return h[:16]  # short id\n",
    "\n",
    "# ---------- RunLogger ----------\n",
    "\n",
    "@dataclass\n",
    "class RunLogger:\n",
    "    root_dir: str\n",
    "    run_id: str\n",
    "    dir_logs: str\n",
    "    dir_meta: str\n",
    "    dir_bundle: str\n",
    "    summary_path: str\n",
    "    jsonl_path: str\n",
    "\n",
    "    @staticmethod\n",
    "    def start(workspace_root: str, tag: str = \"session\") -> \"RunLogger\":\n",
    "        rid = f\"{tag}_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}_{uuid.uuid4().hex[:6]}\"\n",
    "        logs = _mkdirp(os.path.join(workspace_root, \"reports\", \"logs\", rid))\n",
    "        meta = _mkdirp(os.path.join(workspace_root, \"reports\", \"meta\", rid))\n",
    "        bund = _mkdirp(os.path.join(workspace_root, \"reports\", \"bundles\", rid))\n",
    "        return RunLogger(\n",
    "            root_dir=workspace_root,\n",
    "            run_id=rid,\n",
    "            dir_logs=logs,\n",
    "            dir_meta=meta,\n",
    "            dir_bundle=bund,\n",
    "            summary_path=os.path.join(meta, \"summary.json\"),\n",
    "            jsonl_path=os.path.join(logs, \"events.jsonl\"),\n",
    "        )\n",
    "\n",
    "    # event logging (append JSON lines)\n",
    "    def log_event(self, kind: str, payload: Dict[str, Any]):\n",
    "        evt = {\"ts\": _now_iso(), \"run_id\": self.run_id, \"kind\": kind, **payload}\n",
    "        with open(self.jsonl_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(evt, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # param/metric helpers\n",
    "    def log_params(self, step: str, params: Dict[str, Any], code_versions: Optional[Dict[str,str]] = None):\n",
    "        digest = processing_digest(step, code_versions=code_versions or {}, params=params)\n",
    "        self.log_event(\"params\", {\"step\": step, \"digest\": digest, \"params\": params, \"code_versions\": code_versions})\n",
    "\n",
    "    def log_metrics(self, step: str, metrics: Dict[str, Any]):\n",
    "        self.log_event(\"metrics\", {\"step\": step, \"metrics\": metrics})\n",
    "\n",
    "    def log_artifact(self, kind: str, path: str, extra: Optional[Dict[str,Any]] = None):\n",
    "        payload = {\"kind\": kind, \"path\": os.path.abspath(path)}\n",
    "        if extra: payload.update(extra)\n",
    "        self.log_event(\"artifact\", payload)\n",
    "\n",
    "    # summary (overwrites)\n",
    "    def write_summary(self, summary: Dict[str, Any]):\n",
    "        with open(self.summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---------- provenance for files & configs ----------\n",
    "\n",
    "def capture_file_provenance(path: str, role: str = \"input\") -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"role\": role,\n",
    "        \"path\": os.path.abspath(path),\n",
    "        \"sha256\": file_sha256(path),\n",
    "        \"bytes\": os.path.getsize(path),\n",
    "        \"mtime\": int(os.path.getmtime(path)),\n",
    "    }\n",
    "\n",
    "def capture_config_snapshot(config: Dict[str, Any], name: str = \"config\") -> Dict[str, Any]:\n",
    "    snap = {\"name\": name, \"sha256\": json_sha256(config), \"config\": config}\n",
    "    return snap\n",
    "\n",
    "# ---------- reproducibility bundle ----------\n",
    "\n",
    "def make_repro_zip(\n",
    "    out_zip_path: str,\n",
    "    *,\n",
    "    workspace_root: str,\n",
    "    run_logger: RunLogger,\n",
    "    env_info: Dict[str, Any],\n",
    "    inputs: List[str],\n",
    "    outputs: List[str],\n",
    "    extra_jsons: Optional[Dict[str, Any]] = None,\n",
    "    readme_text: Optional[str] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Creates a zip with:\n",
    "      - logs/events.jsonl, meta/summary.json\n",
    "      - environment.json (+ pip_freeze)\n",
    "      - file provenance for inputs/outputs\n",
    "      - any extra JSON config you pass in\n",
    "    Returns absolute path to the zip.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_zip_path)), exist_ok=True)\n",
    "\n",
    "    prov_inputs = [capture_file_provenance(p, role=\"input\") for p in inputs]\n",
    "    prov_outputs = [capture_file_provenance(p, role=\"output\") for p in outputs]\n",
    "\n",
    "    bundle_meta = {\n",
    "        \"created\": _now_iso(),\n",
    "        \"run_id\": run_logger.run_id,\n",
    "        \"workspace_root\": os.path.abspath(workspace_root),\n",
    "        \"environment\": env_info,\n",
    "        \"inputs\": prov_inputs,\n",
    "        \"outputs\": prov_outputs,\n",
    "    }\n",
    "    if extra_jsons:\n",
    "        bundle_meta.update(extra_jsons)\n",
    "\n",
    "    # write temp files in bundle dir for consistent names\n",
    "    env_json = os.path.join(run_logger.dir_meta, \"environment.json\")\n",
    "    with open(env_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(env_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    prov_json = os.path.join(run_logger.dir_meta, \"provenance.json\")\n",
    "    with open(prov_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"inputs\": prov_inputs, \"outputs\": prov_outputs}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    bundle_json = os.path.join(run_logger.dir_meta, \"bundle_meta.json\")\n",
    "    with open(bundle_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle_meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    readme_md = os.path.join(run_logger.dir_meta, \"README.txt\")\n",
    "    with open(readme_md, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(textwrap.dedent(readme_text or f\"\"\"\n",
    "        Post-Mix Reproducibility Bundle\n",
    "        =================================\n",
    "        Run ID: {run_logger.run_id}\n",
    "        Created: {_now_iso()}\n",
    "\n",
    "        Contents:\n",
    "          - logs/events.jsonl      : structured event log\n",
    "          - meta/summary.json      : high-level run summary\n",
    "          - meta/environment.json  : Python/OS/pkg versions (+pip freeze when available)\n",
    "          - meta/provenance.json   : input/output file hashes and sizes\n",
    "          - meta/bundle_meta.json  : collected metadata for quick inspection\n",
    "\n",
    "        To reproduce:\n",
    "          1) Recreate Python env from pip_freeze (if present).\n",
    "          2) Use the same inputs (verified by sha256) and configs.\n",
    "          3) Run via the same code versions; dials/params are in events.jsonl and summary.json.\n",
    "        \"\"\").strip() + \"\\n\")\n",
    "\n",
    "    # zip it up\n",
    "    with zipfile.ZipFile(out_zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        # logs + meta\n",
    "        for root in [run_logger.dir_logs, run_logger.dir_meta]:\n",
    "            for dirpath, _, filenames in os.walk(root):\n",
    "                for fn in filenames:\n",
    "                    ap = os.path.join(dirpath, fn)\n",
    "                    arc = os.path.relpath(ap, os.path.dirname(run_logger.dir_logs))\n",
    "                    z.write(ap, arcname=arc)\n",
    "\n",
    "    return os.path.abspath(out_zip_path)\n",
    "\n",
    "# ---------- convenience: wire into your Manifest ----------\n",
    "\n",
    "def register_and_log_artifact(manifest, logger: RunLogger, path: str, kind: str, params: Optional[Dict[str,Any]] = None, stage: Optional[str] = None):\n",
    "    register_artifact(manifest, path, kind=kind, params=params or {}, stage=stage)\n",
    "    logger.log_artifact(kind, path, extra={\"stage\": stage, \"params\": params or {}})\n",
    "\n",
    "# ---------- version stamps for your code layers (edit these in each layer once) ----------\n",
    "\n",
    "CODE_VERSIONS = {\n",
    "    \"io_layer\":           \"v0.1.0\",\n",
    "    \"analysis_layer\":     \"v0.3.0\",\n",
    "    \"dsp_primitives\":     \"v0.4.1\",\n",
    "    \"processors\":         \"v0.5.0\",\n",
    "    \"render_engine\":      \"v0.2.0\",\n",
    "    \"premaster_prep\":     \"v0.1.0\",\n",
    "    \"orchestrator\":       \"v0.1.0\",\n",
    "    \"stream_sim\":         \"v0.1.0\",\n",
    "    \"compare_reporting\":  \"v0.1.0\",\n",
    "    \"presets_recs\":       \"v0.1.0\",\n",
    "}\n",
    "\n",
    "print(\"Logging · Versioning · Reproducibility layer loaded:\")\n",
    "print(\"- RunLogger.start(workspace_root, tag) → logger\")\n",
    "print(\"- logger.log_params/metrics/artifact(), logger.write_summary()\")\n",
    "print(\"- capture_environment(), file_sha256(), capture_config_snapshot()\")\n",
    "print(\"- processing_digest(name, code_versions, params)\")\n",
    "print(\"- make_repro_zip(out_zip, workspace_root, logger, env_info, inputs, outputs)\")\n",
    "print(\"- register_and_log_artifact(manifest, logger, path, kind, params, stage)\")\n",
    "print(\"- CODE_VERSIONS dict (update per layer when you change code)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232911c-8b5a-4167-9426-a5254749adfa",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4ef09bf-a02c-4430-992f-c1e9c89d8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Utility: exports + true-peak guard ===\n",
    "import os, numpy as np, soundfile as sf\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Use your existing TP approx if available; else fallback\n",
    "try:\n",
    "    true_peak_dbfs\n",
    "except NameError:\n",
    "    from scipy import signal\n",
    "    def true_peak_dbfs(x: np.ndarray, sr: int, oversample: int = 4) -> float:\n",
    "        x_os = signal.resample_poly(np.asarray(x, dtype=np.float32), oversample, 1, axis=0 if np.asarray(x).ndim>1 else 0)\n",
    "        tp = float(np.max(np.abs(x_os)))\n",
    "        return 20.0*np.log10(max(1e-12, tp))\n",
    "\n",
    "def _db_to_lin(db: float) -> float: return 10.0**(db/20.0)\n",
    "\n",
    "def save_wav_24bit(path: str, y: np.ndarray, sr: int):\n",
    "    \"\"\"Always export 24-bit PCM with dirs created.\"\"\"\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
    "    sf.write(path, np.asarray(y, dtype=np.float32), int(sr), subtype=\"PCM_24\")\n",
    "    return os.path.abspath(path)\n",
    "\n",
    "@dataclass\n",
    "class TPGuardResult:\n",
    "    out: np.ndarray\n",
    "    gain_db: float\n",
    "    in_dbtp: float\n",
    "    out_dbtp: float\n",
    "    ceiling_db: float\n",
    "\n",
    "def safe_true_peak(y: np.ndarray, sr: int, ceiling_db: float = -1.0) -> TPGuardResult:\n",
    "    \"\"\"Trim overall gain so true-peak ≤ ceiling (no limiting).\"\"\"\n",
    "    tp_in = true_peak_dbfs(y, sr, oversample=4)\n",
    "    gain_db = ceiling_db - tp_in\n",
    "    y2 = (np.asarray(y, dtype=np.float32) * _db_to_lin(gain_db)).astype(np.float32)\n",
    "    tp_out = true_peak_dbfs(y2, sr, oversample=4)\n",
    "    return TPGuardResult(out=y2, gain_db=gain_db, in_dbtp=tp_in, out_dbtp=tp_out, ceiling_db=ceiling_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56f9a54c-bb52-46f0-8450-3a8a70f3e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Global config + validators ===\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    # I/O\n",
    "    default_bit_depth: str = \"PCM_24\"\n",
    "    # Prep\n",
    "    prep_hpf_hz: float = 20.0\n",
    "    prep_peak_target_dbfs: float = -6.0\n",
    "    # Rendering\n",
    "    render_peak_target_dbfs: float = -1.0\n",
    "    # Streaming sim\n",
    "    tp_ceiling_db: float = -1.0\n",
    "    # Reporting\n",
    "    preview_seconds: int = 60\n",
    "    nfft: int = 1<<16\n",
    "\n",
    "CFG = GlobalConfig()\n",
    "\n",
    "class InputError(Exception): pass\n",
    "\n",
    "def ensure_audio_valid(x: np.ndarray, name: str = \"audio\"):\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        raise InputError(f\"{name}: expected numpy array, got {type(x)}\")\n",
    "    if x.ndim not in (1,2):\n",
    "        raise InputError(f\"{name}: expected 1D (mono) or 2D (stereo), got shape {x.shape}\")\n",
    "    if not np.isfinite(x).all():\n",
    "        raise InputError(f\"{name}: contains NaN/Inf values; sanitize before processing\")\n",
    "    if x.size == 0:\n",
    "        raise InputError(f\"{name}: empty array\")\n",
    "    if x.ndim == 2 and x.shape[1] not in (1,2):\n",
    "        raise InputError(f\"{name}: expected (N,), (N,1) or (N,2); got {x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d4830ec-46c5-47a4-af26-2c512d7b74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reporting helpers (enhanced HTML) ===\n",
    "import os, io, html\n",
    "from typing import Optional, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "def df_with_links(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"If 'path' column exists, add a clickable 'file' column for HTML report.\"\"\"\n",
    "    if \"path\" in df.columns:\n",
    "        def mk_link(p):\n",
    "            base = os.path.basename(str(p))\n",
    "            return f\"<a href='{html.escape(base)}' target='_blank'>{html.escape(base)}</a>\"\n",
    "        df2 = df.copy()\n",
    "        df2[\"file\"] = df2[\"path\"].apply(mk_link)\n",
    "        # put 'file' right after 'name' if present\n",
    "        cols = list(df2.columns)\n",
    "        if \"name\" in cols:\n",
    "            cols.insert(cols.index(\"name\")+1, cols.pop(cols.index(\"file\")))\n",
    "            df2 = df2[cols]\n",
    "        return df2\n",
    "    return df\n",
    "\n",
    "def html_header_block(title: str, code_versions: Dict[str,str], dials: Optional[Dict[str,Any]] = None, notes: Optional[str] = None) -> str:\n",
    "    buf = io.StringIO()\n",
    "    buf.write(f\"<h1>{html.escape(title)}</h1>\\n\")\n",
    "    if notes:\n",
    "        buf.write(f\"<p><em>{html.escape(notes)}</em></p>\\n\")\n",
    "    # versions\n",
    "    if code_versions:\n",
    "        buf.write(\"<h3>Code Versions</h3><ul>\")\n",
    "        for k,v in code_versions.items():\n",
    "            buf.write(f\"<li><code>{html.escape(k)}</code>: {html.escape(v)}</li>\")\n",
    "        buf.write(\"</ul>\")\n",
    "    # dials snapshot\n",
    "    if dials:\n",
    "        buf.write(\"<h3>Dial Snapshot</h3><ul>\")\n",
    "        for k,v in dials.items():\n",
    "            buf.write(f\"<li>{html.escape(k)}: {html.escape(str(v))}</li>\")\n",
    "        buf.write(\"</ul>\")\n",
    "    return buf.getvalue()\n",
    "\n",
    "# Patch your write_report_html to use these (drop-in):\n",
    "def write_report_html_enhanced(\n",
    "    summary_df: pd.DataFrame,\n",
    "    deltas_df: Optional[pd.DataFrame],\n",
    "    plots: Dict[str, str],\n",
    "    out_path: str,\n",
    "    title: str = \"Post-Mix Comparison Report\",\n",
    "    extra_notes: Optional[str] = None,\n",
    "    code_versions: Optional[Dict[str,str]] = None,\n",
    "    dial_snapshot: Optional[Dict[str,Any]] = None\n",
    ") -> str:\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_path)), exist_ok=True)\n",
    "    # linkify\n",
    "    summary_df2 = df_with_links(summary_df)\n",
    "    # base HTML\n",
    "    html_doc = io.StringIO()\n",
    "    html_doc.write(f\"<!doctype html><html><head><meta charset='utf-8'><title>{title}</title>\")\n",
    "    html_doc.write(\"<style>body{font-family:system-ui,Arial,sans-serif;margin:24px} h1{margin-top:0} img{max-width:100%;height:auto} table{border-collapse:collapse} th,td{border:1px solid #ddd;padding:6px} code{background:#f5f5f5;padding:2px 4px;border-radius:4px}</style>\")\n",
    "    html_doc.write(\"</head><body>\")\n",
    "    html_doc.write(html_header_block(title, code_versions or {}, dials=dial_snapshot, notes=extra_notes))\n",
    "    # tables\n",
    "    html_doc.write(\"<h3>Summary Metrics</h3>\")\n",
    "    html_doc.write(summary_df2.to_html(index=False, escape=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "    if deltas_df is not None and len(deltas_df):\n",
    "        html_doc.write(\"<h3>Δ vs Reference</h3>\")\n",
    "        html_doc.write(deltas_df.to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "    # plots\n",
    "    if \"spectrum_png\" in plots and os.path.exists(plots[\"spectrum_png\"]):\n",
    "        html_doc.write(\"<h3>Spectrum Overlay</h3>\")\n",
    "        html_doc.write(f\"<img src='{os.path.basename(plots['spectrum_png'])}' alt='Spectrum Overlay'/>\")\n",
    "    if \"loudness_png\" in plots and os.path.exists(plots[\"loudness_png\"]):\n",
    "        html_doc.write(\"<h3>Short-Term Loudness Overlay</h3>\")\n",
    "        html_doc.write(f\"<img src='{os.path.basename(plots['loudness_png'])}' alt='Loudness Overlay'/>\")\n",
    "    html_doc.write(\"</body></html>\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_doc.getvalue())\n",
    "    # copy assets next to HTML (same as before)\n",
    "    import shutil\n",
    "    for p in plots.values():\n",
    "        if os.path.exists(p):\n",
    "            shutil.copy2(p, os.path.join(os.path.dirname(out_path), os.path.basename(p)))\n",
    "    return os.path.abspath(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b5769-f950-4a11-8a0c-8960e9447013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a93e5088-09cc-43c4-a409-5454a931497c",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c9eb2e5-a7eb-4b46-ba27-62bbd1661b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Patch: make report writers skip same-file copies ----\n",
    "import os, shutil\n",
    "\n",
    "def _safe_copy_to_dir(src_path: str, target_dir: str):\n",
    "    \"\"\"Copy src_path into target_dir unless it's already there.\"\"\"\n",
    "    if not os.path.exists(src_path):\n",
    "        return None\n",
    "    dst = os.path.join(target_dir, os.path.basename(src_path))\n",
    "    try:\n",
    "        # if already same path or same inode → skip\n",
    "        if os.path.abspath(src_path) == os.path.abspath(dst) or (\n",
    "            os.path.exists(dst) and os.path.samefile(src_path, dst)\n",
    "        ):\n",
    "            return dst\n",
    "    except Exception:\n",
    "        # os.path.samefile might fail on some platforms; fall through to copy\n",
    "        pass\n",
    "    shutil.copy2(src_path, dst)\n",
    "    return dst\n",
    "\n",
    "# Rebind write_report_html to use safe copy\n",
    "def write_report_html(summary_df, deltas_df, plots, out_path, title=\"Post-Mix Comparison Report\", extra_notes=None):\n",
    "    import io\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_path)), exist_ok=True)\n",
    "    html = io.StringIO()\n",
    "    html.write(f\"<!doctype html><html><head><meta charset='utf-8'><title>{title}</title>\")\n",
    "    html.write(\"<style>body{font-family:system-ui,Arial,sans-serif;margin:24px} h1{margin-top:0} img{max-width:100%;height:auto} table{border-collapse:collapse} th,td{border:1px solid #ddd;padding:6px} caption{margin:6px 0}</style>\")\n",
    "    html.write(\"</head><body>\")\n",
    "    html.write(f\"<h1>{title}</h1>\")\n",
    "    if extra_notes:\n",
    "        html.write(f\"<p><em>{extra_notes}</em></p>\")\n",
    "    # tables\n",
    "    html.write(summary_df.to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "    if deltas_df is not None and len(deltas_df):\n",
    "        html.write(deltas_df.to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "    # plots\n",
    "    if \"spectrum_png\" in plots and os.path.exists(plots[\"spectrum_png\"]):\n",
    "        html.write(\"<h3>Spectrum Overlay</h3>\")\n",
    "        html.write(f\"<img src='{os.path.basename(plots['spectrum_png'])}' alt='Spectrum Overlay'/>\")\n",
    "    if \"loudness_png\" in plots and os.path.exists(plots[\"loudness_png\"]):\n",
    "        html.write(\"<h3>Short-Term Loudness Overlay</h3>\")\n",
    "        html.write(f\"<img src='{os.path.basename(plots['loudness_png'])}' alt='Loudness Overlay'/>\")\n",
    "    html.write(\"</body></html>\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html.getvalue())\n",
    "\n",
    "    # copy assets next to HTML, but skip if already there\n",
    "    target_dir = os.path.dirname(out_path)\n",
    "    for p in plots.values():\n",
    "        if p:\n",
    "            _safe_copy_to_dir(p, target_dir)\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "# If you use the enhanced writer, patch its copy loop too\n",
    "try:\n",
    "    _old_enh = write_report_html_enhanced  # if defined earlier\n",
    "\n",
    "    def write_report_html_enhanced(summary_df, deltas_df, plots, out_path,\n",
    "                                   title=\"Post-Mix Comparison Report\",\n",
    "                                   extra_notes=None, code_versions=None, dial_snapshot=None):\n",
    "        import io, html\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(out_path)), exist_ok=True)\n",
    "\n",
    "        # (keep your previous enhanced content generation…)\n",
    "        html_doc = io.StringIO()\n",
    "        html_doc.write(f\"<!doctype html><html><head><meta charset='utf-8'><title>{title}</title>\")\n",
    "        html_doc.write(\"<style>body{font-family:system-ui,Arial,sans-serif;margin:24px} h1{margin-top:0} img{max-width:100%;height:auto} table{border-collapse:collapse} th,td{border:1px solid #ddd;padding:6px} code{background:#f5f5f5;padding:2px 4px;border-radius:4px}</style>\")\n",
    "        html_doc.write(\"</head><body>\")\n",
    "        # simple header\n",
    "        html_doc.write(f\"<h1>{html.escape(title)}</h1>\")\n",
    "        if extra_notes:\n",
    "            html_doc.write(f\"<p><em>{html.escape(extra_notes)}</em></p>\")\n",
    "        if code_versions:\n",
    "            html_doc.write(\"<h3>Code Versions</h3><ul>\")\n",
    "            for k,v in code_versions.items():\n",
    "                html_doc.write(f\"<li><code>{html.escape(k)}</code>: {html.escape(str(v))}</li>\")\n",
    "            html_doc.write(\"</ul>\")\n",
    "        if dial_snapshot:\n",
    "            html_doc.write(\"<h3>Dial Snapshot</h3><ul>\")\n",
    "            for k,v in dial_snapshot.items():\n",
    "                html_doc.write(f\"<li>{html.escape(k)}: {html.escape(str(v))}</li>\")\n",
    "            html_doc.write(\"</ul>\")\n",
    "\n",
    "        html_doc.write(\"<h3>Summary Metrics</h3>\")\n",
    "        html_doc.write(summary_df.to_html(index=False, escape=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "        if deltas_df is not None and len(deltas_df):\n",
    "            html_doc.write(\"<h3>Δ vs Reference</h3>\")\n",
    "            html_doc.write(deltas_df.to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\n",
    "\n",
    "        if \"spectrum_png\" in plots and os.path.exists(plots[\"spectrum_png\"]):\n",
    "            html_doc.write(\"<h3>Spectrum Overlay</h3>\")\n",
    "            html_doc.write(f\"<img src='{os.path.basename(plots['spectrum_png'])}' alt='Spectrum Overlay'/>\")\n",
    "        if \"loudness_png\" in plots and os.path.exists(plots[\"loudness_png\"]):\n",
    "            html_doc.write(\"<h3>Short-Term Loudness Overlay</h3>\")\n",
    "            html_doc.write(f\"<img src='{os.path.basename(plots['loudness_png'])}' alt='Loudness Overlay'/>\")\n",
    "\n",
    "        html_doc.write(\"</body></html>\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_doc.getvalue())\n",
    "\n",
    "        # safe-copy assets\n",
    "        target_dir = os.path.dirname(out_path)\n",
    "        for p in plots.values():\n",
    "            if p:\n",
    "                _safe_copy_to_dir(p, target_dir)\n",
    "        return os.path.abspath(out_path)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da90abf6-4396-485b-be68-b5048693bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN: end-to-end sanity test for I/O → Analysis → DSP Primitives → Processors\n",
    "# Run this cell after loading the previous cells.\n",
    "# It will:\n",
    "#  - create a workspace\n",
    "#  - import your mix (set MIX_SRC below)\n",
    "#  - analyze the original\n",
    "#  - build a preview cache\n",
    "#  - render a few dial presets\n",
    "#  - save outputs, register artifacts\n",
    "#  - compare metrics in a table\n",
    "#  - plot spectrum & loudness overlays\n",
    "#\n",
    "# Safe to re-run; it creates a new timestamped workspace each time.\n",
    "\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------- 1) SET YOUR SOURCE FILE HERE ----------\n",
    "MIX_SRC = \"/Users/itay/Documents/muxing/ITAY - CRASHING v.6 MIX ONLY.wav\"   # <-- change me\n",
    "MIX_SRC = \"/Users/itay/Documents/muxing/ITAY - 4 CHORDS v.21 STEFAN.wav\"   # <-- change me\n",
    "MIX_SRC = \"/Users/itay/Documents/muxing/Full Song-jerrysmith1984-C7F08-SONGIVA-V4.wav\"   # <-- change me\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d723620e-2113-4240-a577-a45148c0b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace created at: /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354\n",
      "Imported mix → /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/inputs/mix.wav\n",
      "Original Mix: sr=48000 | ch=2 | dur=356.333s | peak=0.928901 | rms=0.251534\n",
      "  path: /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/inputs/mix.wav\n",
      "  sha256: f6086ee74bf35fbf...\n",
      "  src dtype: int32 | src ch: 2\n",
      "- Balanced Gentle (priority 9): B15 P15 C10 A10 W5\n",
      "    • No strong issues detected → start with light, broad polish.\n",
      "Manifest written: /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/manifest.json\n",
      "\n",
      "=== DONE ===\n",
      "Workspace: /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354\n",
      "Premaster (for mastering): /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/premaster/premaster_for_mastering.wav\n",
      "Masters:\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/master/local_neutral_50.wav\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/master/local_warm_50.wav\n",
      "As-heard previews:\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/stream_previews/local_neutral_50__asheard_Spotify.wav\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/stream_previews/local_neutral_50__asheard_AppleMusic.wav\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/stream_previews/local_neutral_50__asheard_YouTube.wav\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/stream_previews/local_neutral_50__asheard_TIDAL.wav\n",
      " - /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/outputs/stream_previews/local_neutral_50__asheard_Amazon.wav\n",
      "Report: /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/reports/assets/comparison_report.html\n",
      "Repro bundle: /Users/itay/Documents/muxing/postmix_runs/postmix_v1_20250820-134354/reports/bundles/postmix_v1_20250820T134354Z_cfe489.zip\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MAIN — Full End-to-End Pipeline (fixed report paths)\n",
    "# ============================================\n",
    "# Saves plots AND the HTML report inside reports/assets/ to avoid SameFileError.\n",
    "\n",
    "import os, json, numpy as np, soundfile as sf\n",
    "from dataclasses import asdict\n",
    "\n",
    "# ---------- 0) SET YOUR SOURCE FILE HERE ----------\n",
    "PROJECT  = \"postmix_v1\"  # MIX_SRC must be set in a previous cell\n",
    "\n",
    "# ---------- 1) Global toggles ----------\n",
    "DO_STREAMING_SIM = True\n",
    "RECS_LIMIT       = 3\n",
    "LM_PREVIEW_LUFS  = -14.0\n",
    "REPORT_REF_NAME  = \"Original\"\n",
    "\n",
    "# ---------- 2) Workspace, manifest, logger, environment ----------\n",
    "paths = make_workspace(project=PROJECT)\n",
    "man   = Manifest(project=PROJECT, workspace=paths)\n",
    "\n",
    "logger = RunLogger.start(paths.root, tag=PROJECT)\n",
    "env    = capture_environment()\n",
    "logger.log_event(\"env\", {\"environment\": env})\n",
    "\n",
    "# bring input into workspace + register/log\n",
    "if not os.path.exists(MIX_SRC):\n",
    "    raise FileNotFoundError(f\"Set MIX_SRC to a valid path, got: {MIX_SRC}\")\n",
    "mix_path = import_mix(paths, MIX_SRC, alias=\"mix.wav\")\n",
    "register_input(man, mix_path, alias=\"mix\")\n",
    "register_and_log_artifact(man, logger, mix_path, kind=\"input\", params={\"alias\":\"mix\"}, stage=\"import_mix\")\n",
    "\n",
    "# ---------- 3) Load, validate, analyze original ----------\n",
    "x, sr = sf.read(mix_path)\n",
    "ensure_audio_valid(x, \"mix\")\n",
    "\n",
    "buf = load_wav(mix_path)\n",
    "print_audio_summary(buf, \"Original Mix\")\n",
    "\n",
    "rep_orig = analyze_wav(mix_path)\n",
    "logger.log_metrics(\"analysis_original\", {\n",
    "    \"sr\": rep_orig.sr, \"duration_s\": rep_orig.duration_s,\n",
    "    \"peak_dbfs\": rep_orig.basic[\"peak_dbfs\"],\n",
    "    \"true_peak_dbfs\": rep_orig.true_peak_dbfs,\n",
    "    \"rms_dbfs\": rep_orig.basic[\"rms_dbfs\"],\n",
    "    \"lufs_int\": rep_orig.lufs_integrated,\n",
    "    \"crest_db\": rep_orig.basic[\"crest_db\"],\n",
    "    \"bass_%\": rep_orig.bass_energy_pct,\n",
    "    \"air_%\": rep_orig.air_energy_pct,\n",
    "    \"phase_corr\": rep_orig.stereo[\"phase_correlation\"],\n",
    "    \"stereo_width\": rep_orig.stereo[\"stereo_width\"],\n",
    "    \"spectral_flatness\": rep_orig.spectral_flatness,\n",
    "})\n",
    "\n",
    "# ---------- 4) Recommendations (dials) ----------\n",
    "recs = recommend_from_analysis(rep_orig)\n",
    "print(recommendation_summary(recs))\n",
    "\n",
    "variants_plan = build_premaster_plan_from_recs(recs, limit=RECS_LIMIT, prefix=\"PM\")\n",
    "variants_plan.insert(0, (\"PM0_Transparent\", DialState(bass=0, punch=0, clarity=0, air=0, width=0)))\n",
    "logger.log_params(\"recommendations\", {\n",
    "    \"plan\": [(name, asdict(d)) for (name, d) in variants_plan]\n",
    "}, code_versions={\"presets_recs\": CODE_VERSIONS[\"presets_recs\"]})\n",
    "\n",
    "# ---------- 5) RenderEngine: preprocess + premaster variants ----------\n",
    "engine = RenderEngine(x, sr, preprocess=PreprocessConfig(low_cutoff=120.0, kick_lo=40.0, kick_hi=110.0))\n",
    "pre_meta = engine.preprocess()\n",
    "logger.log_params(\"render_preprocess_cache\", pre_meta, code_versions={\"processors\": CODE_VERSIONS[\"processors\"], \"render_engine\": CODE_VERSIONS[\"render_engine\"]})\n",
    "\n",
    "# A) classic premaster prep (HPF 20 Hz + peak -6 dBFS)\n",
    "prep_audio, prep_info = premaster_prep(x, sr, target_peak_dbfs=CFG.prep_peak_target_dbfs, hpf_hz=CFG.prep_hpf_hz)\n",
    "premaster_prep_path = os.path.join(paths.outputs, \"premaster\", \"premaster_prep.wav\")\n",
    "os.makedirs(os.path.dirname(premaster_prep_path), exist_ok=True)\n",
    "save_wav_24bit(premaster_prep_path, prep_audio, sr)\n",
    "register_and_log_artifact(man, logger, premaster_prep_path, kind=\"premaster\", params=prep_info, stage=\"premaster_prep\")\n",
    "\n",
    "# B) dial-based premaster variants\n",
    "variant_dir = os.path.join(paths.outputs, \"premasters\")\n",
    "opts = RenderOptions(target_peak_dbfs=CFG.render_peak_target_dbfs, bit_depth=CFG.default_bit_depth, hpf_hz=None, save_headroom_first=False)\n",
    "var_metas = engine.commit_variants(variant_dir, variants_plan, opts=opts)\n",
    "for meta in var_metas:\n",
    "    register_and_log_artifact(man, logger, meta[\"out_path\"], kind=\"premaster\", params=meta, stage=f\"variant__{os.path.basename(meta['out_path'])}\")\n",
    "\n",
    "# Choose a “premaster for mastering”\n",
    "premaster_choice_path = var_metas[1][\"out_path\"] if len(var_metas) > 1 else premaster_prep_path  # skip transparent baseline\n",
    "\n",
    "# true-peak guard before sending to mastering\n",
    "y_in, sr2 = sf.read(premaster_choice_path)\n",
    "tpres = safe_true_peak(y_in, sr2, ceiling_db=CFG.tp_ceiling_db)  # TPGuardResult\n",
    "premaster_for_mastering_path = os.path.join(paths.outputs, \"premaster\", \"premaster_for_mastering.wav\")\n",
    "save_wav_24bit(premaster_for_mastering_path, tpres.out, sr2)\n",
    "tp_meta = {\"gain_db\": float(tpres.gain_db), \"in_dbtp\": float(tpres.in_dbtp), \"out_dbtp\": float(tpres.out_dbtp), \"ceiling_db\": float(tpres.ceiling_db)}\n",
    "register_and_log_artifact(man, logger, premaster_for_mastering_path, kind=\"premaster\", params={\"tp_guard\": tp_meta}, stage=\"premaster_for_mastering\")\n",
    "\n",
    "# ---------- 6) Mastering Orchestrator (Local provider; LANDR stub available) ----------\n",
    "orch = MasteringOrchestrator(paths, man)\n",
    "styles = [(s, strength) for (s, strength, why) in recommend_mastering_styles_from_metrics(analyze_wav(premaster_for_mastering_path))]\n",
    "providers = [LocalMasterProvider(bit_depth=CFG.default_bit_depth)]\n",
    "logger.log_params(\"mastering_styles\", {\"styles\": styles}, code_versions={\"orchestrator\": CODE_VERSIONS[\"orchestrator\"]})\n",
    "\n",
    "master_results = orch.run(\n",
    "    premaster_path=premaster_for_mastering_path,\n",
    "    providers=providers,\n",
    "    styles=styles,\n",
    "    out_tag=\"master\",\n",
    "    level_match_preview_lufs=LM_PREVIEW_LUFS\n",
    ")\n",
    "master_paths = [r.out_path for r in master_results]\n",
    "\n",
    "# ---------- 7) Streaming Normalization Simulator (as-heard files) ----------\n",
    "stream_paths = []\n",
    "stream_df = None\n",
    "if DO_STREAMING_SIM and master_paths:\n",
    "    stream_outdir = os.path.join(paths.outputs, \"stream_previews\")\n",
    "    stream_paths, stream_df = simulate_and_export_for_platforms(\n",
    "        input_path=master_paths[0],\n",
    "        out_dir=stream_outdir,\n",
    "        profiles=default_streaming_profiles(),\n",
    "        bit_depth=CFG.default_bit_depth,\n",
    "        register_to_manifest=(man, \"stream_sim\")\n",
    "    )\n",
    "    logger.log_event(\"stream_sim_summary\", {\"rows\": None if stream_df is None else len(stream_df)})\n",
    "\n",
    "# ---------- 8) Reporting (metrics, plots, HTML) ----------\n",
    "compare_files = [mix_path, premaster_for_mastering_path] + master_paths\n",
    "compare_files = [p for p in compare_files if os.path.exists(p)]\n",
    "\n",
    "cfg_cmp = CompareConfig(\n",
    "    preview_seconds=CFG.preview_seconds,\n",
    "    nfft=CFG.nfft,\n",
    "    reference_name=REPORT_REF_NAME\n",
    ")\n",
    "\n",
    "# Save plots AND HTML into reports/assets/ (no cross-copy collisions)\n",
    "reports_assets_dir = os.path.join(paths.reports, \"assets\")\n",
    "os.makedirs(reports_assets_dir, exist_ok=True)\n",
    "\n",
    "bundle = write_report_bundle(\n",
    "    file_paths=compare_files,\n",
    "    reports_dir=reports_assets_dir,               # plots + HTML here\n",
    "    cfg=cfg_cmp,\n",
    "    manifest=man,\n",
    "    report_name=\"comparison_report.html\",         # filename only\n",
    "    extra_notes=\"Auto-generated end-to-end run.\"\n",
    ")\n",
    "\n",
    "# Optional enhanced HTML (keep it in the same assets dir)\n",
    "try:\n",
    "    html_enh = write_report_html_enhanced(\n",
    "        bundle[\"summary_df\"], bundle[\"deltas_df\"], bundle[\"plots\"],\n",
    "        os.path.join(reports_assets_dir, \"comparison_report_enhanced.html\"),\n",
    "        title=\"Post-Mix Comparison Report\",\n",
    "        extra_notes=\"Client review pack\",\n",
    "        code_versions=CODE_VERSIONS,\n",
    "        dial_snapshot=variants_plan[1][1].__dict__ if len(variants_plan)>1 else {}\n",
    "    )\n",
    "    register_and_log_artifact(man, logger, html_enh, kind=\"report\", params={\"enhanced\": True}, stage=\"compare_html_enhanced\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# ---------- 9) Reproducibility bundle ----------\n",
    "zip_path = os.path.join(paths.reports, \"bundles\", f\"{logger.run_id}.zip\")\n",
    "repro_zip = make_repro_zip(\n",
    "    zip_path,\n",
    "    workspace_root=paths.root,\n",
    "    run_logger=logger,\n",
    "    env_info=env,\n",
    "    inputs=[mix_path],\n",
    "    outputs=list(set([premaster_prep_path, premaster_for_mastering_path] + [m for m in master_paths] + stream_paths)),\n",
    "    extra_jsons={\"code_versions\": CODE_VERSIONS, \"recommendations\": [(n, asdict(d)) for (n,d) in variants_plan]},\n",
    "    readme_text=\"Bundle created by MAIN end-to-end run.\"\n",
    ")\n",
    "register_and_log_artifact(man, logger, repro_zip, kind=\"bundle\", params={\"run_id\": logger.run_id}, stage=\"repro_zip\")\n",
    "\n",
    "# ---------- 10) Finalize ----------\n",
    "logger.write_summary({\n",
    "    \"project\": PROJECT,\n",
    "    \"run_id\": logger.run_id,\n",
    "    \"mix\": mix_path,\n",
    "    \"premaster_choice\": premaster_for_mastering_path,\n",
    "    \"masters\": master_paths,\n",
    "    \"stream_previews\": stream_paths,\n",
    "    \"report\": bundle[\"html_path\"],   # lives in reports/assets/\n",
    "    \"env_sha\": json_sha256(env),\n",
    "})\n",
    "write_manifest(man)\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Workspace:\", paths.root)\n",
    "print(\"Premaster (for mastering):\", premaster_for_mastering_path)\n",
    "print(\"Masters:\", *master_paths, sep=\"\\n - \")\n",
    "if stream_paths: print(\"As-heard previews:\", *stream_paths, sep=\"\\n - \")\n",
    "print(\"Report:\", bundle[\"html_path\"])\n",
    "print(\"Repro bundle:\", repro_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4429fa8-e910-4051-9ae7-9925d23e2982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe4973-f4e5-490b-aaed-33312aa61439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b883-3667-483f-8860-b5448050cead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (audio-sim)",
   "language": "python",
   "name": "audio-sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
